

Source files copied in : md/Source/
Generated source files : md/src
###################### md/src/COMPUTE_PARDO1.f ######################
!!
!! file for COMPUTE_PARDO1.f
!!
      SUBROUTINE COMPUTE_PARDO1(I, I_L, I_U, L, ND, NP, F, J, BOX, 
     &POS, RIJ, D, POT, PI2, K, KIN, VEL)
      REAL*8 DOTR8
      EXTERNAL DOTR8
      INTEGER I, I_L, I_U, L, ND, NP, J, K
      REAL*8 F(1:ND, 1:NP), BOX(1:ND), POS(1:ND, 1:NP), RIJ(1:ND), D
     &, POT, PI2, KIN, VEL(1:ND, 1:NP)

      DO I = I_L, I_U                                                   0009
! compute potential energy and forces

!!        f(1:nd,i) = 0.0
         DO L = 1, ND                                                   0013
            F(L,I) = 0.0                                                0014
         ENDDO


         DO J = 1, NP                                                   0018
            IF (I.NE.J) THEN                                            0019
               CALL DIST(ND, BOX, POS(1,I), POS(1,J), RIJ, D)           0020
! attribute half of the potential energy to particle 'j'

               POT = POT+0.5*SIN(MIN(D, PI2))**2.                       0023
               DO K = 1, ND                                             0024
                  F(K,I) = F(K,I)-RIJ(K)*2.*SIN(MIN(D, PI2))*COS(MIN(   0025
     &            D, PI2))/D                                            0025
               ENDDO
            ENDIF
         ENDDO
! compute kinetic energy

         KIN = KIN+DOTR8(ND, VEL(1,I), VEL(1,I))                        0032
      ENDDO
      END
###################### md/src/COMPUTE_PARDO1_MPI.f ######################
!!
!! file for COMPUTE_PARDO1_MPI.f
!!
      SUBROUTINE COMPUTE_PARDO1_MPI(I, I_L, I_U, L, ND, NP, F, J, BOX
     &, POS, RIJ, D, POT, PI2, K, KIN, VEL)
      implicit none
      include "STEP.h"
      INTEGER*4 MAX_NB_REQUEST
      PARAMETER (MAX_NB_REQUEST = 2*3*MAX_NB_LOOPSLICES)
      INTEGER STEP_I_UP, STEP_I_LOW, I_UP, I_LOW, IDX, I, I_L, I_U, L
     &, ND, NP, J, K
      INTEGER STEP_SR_F(IDX_SLICE_LOW:IDX_SLICE_UP, 1:2, 0:
     &MAX_NB_LOOPSLICES), STEP_I_LOOPSLICES(IDX_SLICE_LOW:
     &IDX_SLICE_UP, 1:MAX_NB_LOOPSLICES), STEP_REQUESTS(1:
     &MAX_NB_REQUEST)
      REAL*8 STEP_KIN_REDUC, STEP_POT_REDUC, F(1:ND, 1:NP), BOX(1:ND)
     &, POS(1:ND, 1:NP), RIJ(1:ND), D, POT, PI2, KIN, VEL(1:ND, 1:NP)
      CALL STEP_INITREDUCTION_R8(POT, STEP_POT_REDUC, STEP_SUM)         0101
      CALL STEP_INITREDUCTION_R8(KIN, STEP_KIN_REDUC, STEP_SUM)         0102

      CALL STEP_COMPUTELOOPSLICES(I_L, I_U, 1, STEP_SIZE,               0104
     &MAX_NB_LOOPSLICES, STEP_I_LOOPSLICES)                             0104

C     Put array boundaries into region arrays (SR: Send region)
C     First dimension: lower and upper bounds of each slice
C     Second dimension: for each dimension of the original array
C     Third dimention: store the boundaries of the local chunk. The first element stores initial boundaries, then one element for each process
      STEP_SR_F(IDX_SLICE_LOW,1,0) = 1                                  0111
      STEP_SR_F(IDX_SLICE_UP,1,0) = ND                                  0112
      STEP_SR_F(IDX_SLICE_LOW,2,0) = 1                                  0113
      STEP_SR_F(IDX_SLICE_UP,2,0) = NP                                  0114

C     Region computation
      DO 99999 IDX = 1, MAX_NB_LOOPSLICES                               0117
         I_LOW = STEP_I_LOOPSLICES(IDX_SLICE_LOW,IDX)                   0118
         I_UP = STEP_I_LOOPSLICES(IDX_SLICE_UP,IDX)                     0119
C  <F(PHI1,PHI2)-write-MAY-{1<=PHI1, PHI1<=ND, PHI1<=3, I_LOW<=PHI2,
C    1<=PHI2, PHI2<=I_UP, PHI2<=500}>
         STEP_SR_F(IDX_SLICE_LOW,1,IDX) = 1                             0122
         STEP_SR_F(IDX_SLICE_UP,1,IDX) = MIN(ND, 3)                     0123
         STEP_SR_F(IDX_SLICE_LOW,2,IDX) = MAX(I_LOW, 1)                 0124
         STEP_SR_F(IDX_SLICE_UP,2,IDX) = MIN(I_UP, 500)                 0125
99999    CONTINUE                                                       0126

C     Where work is done...
      STEP_I_LOW = STEP_I_LOOPSLICES(IDX_SLICE_LOW,STEP_RANK+1)         0129
      STEP_I_UP = STEP_I_LOOPSLICES(IDX_SLICE_UP,STEP_RANK+1)           0130
      CALL COMPUTE_PARDO1(I, STEP_I_LOW, STEP_I_UP, L, ND, NP, F, J,    0131
     &BOX, POS, RIJ, D, POT, PI2, K, KIN, VEL)                          0131

C     Communicating data to other nodes
C     3 communication shemes for all-to-all personalized broadcast :
C     STEP_NONBLOCKING, STEP_BLOCKING1 and STEP_BLOCKING2.
C     A nonblocking algo increment STEP_Nb_Request.
      STEP_NBREQUEST = 0                                                0138
      CALL STEP_ALLTOALLREGION_R8(2, STEP_SIZE, STEP_SR_F,              0139
     &STEP_SIZEREGION(2, STEP_SR_F(IDX_SLICE_LOW,1,0)), F, 0,           0139
     &MAX_NB_REQUEST, STEP_REQUESTS, STEP_NBREQUEST, STEP_NONBLOCKING   0139
     &)                                                                 0139
C     If STEP_Nb_Request equals 0, STEP_WAITALL does nothing
      CALL STEP_WAITALL(STEP_NBREQUEST, STEP_REQUESTS)                  0144
      CALL STEP_REDUCTION_R8(POT, STEP_POT_REDUC, STEP_SUM)             0145
      CALL STEP_REDUCTION_R8(KIN, STEP_KIN_REDUC, STEP_SUM)             0146
      END
###################### md/src/COMPUTE_PARDO1_OMP.f ######################
!!
!! file for COMPUTE_PARDO1_OMP.f
!!
      SUBROUTINE COMPUTE_PARDO1_OMP(I, I_L, I_U, L, ND, NP, F, J, BOX
     &, POS, RIJ, D, POT, PI2, K, KIN, VEL)
      REAL*8 DOTR8
      EXTERNAL DOTR8
      INTEGER I, I_L, I_U, L, ND, NP, J, K
      REAL*8 F(1:ND, 1:NP), BOX(1:ND), POS(1:ND, 1:NP), RIJ(1:ND), D
     &, POT, PI2, KIN, VEL(1:ND, 1:NP)
!$OMP parallel do reduction(+ : POT) reduction(+ : KIN) private(D, RIJ, 
!$OMP&K, J, I)

      DO I = I_L, I_U                                                   0011
! compute potential energy and forces

!!        f(1:nd,i) = 0.0
         DO L = 1, ND                                                   0015
            F(L,I) = 0.0                                                0016
         ENDDO


         DO J = 1, NP                                                   0020
            IF (I.NE.J) THEN                                            0021
               CALL DIST(ND, BOX, POS(1,I), POS(1,J), RIJ, D)           0022
! attribute half of the potential energy to particle 'j'

               POT = POT+0.5*SIN(MIN(D, PI2))**2.                       0025
               DO K = 1, ND                                             0026
                  F(K,I) = F(K,I)-RIJ(K)*2.*SIN(MIN(D, PI2))*COS(MIN(   0027
     &            D, PI2))/D                                            0027
               ENDDO
            ENDIF
         ENDDO
! compute kinetic energy

         KIN = KIN+DOTR8(ND, VEL(1,I), VEL(1,I))                        0034
      ENDDO
!$OMP end parallel do
      END
###################### md/src/MD_MASTER1.f ######################
!!
!! file for MD_MASTER1.f
!!
      SUBROUTINE MD_MASTER1(POTENTIAL, KINETIC, E0)
      REAL*8 POTENTIAL, KINETIC, E0
      PRINT *, POTENTIAL, KINETIC, (POTENTIAL+KINETIC-E0)/E0            0003
      END
###################### md/src/MD_MASTER1_MPI.f ######################
!!
!! file for MD_MASTER1_MPI.f
!!
      SUBROUTINE MD_MASTER1_MPI(POTENTIAL, KINETIC, E0)
      implicit none
      include "STEP.h"
      REAL*8 POTENTIAL, KINETIC, E0
      IF (STEP_RANK.EQ.0) CALL MD_MASTER1(POTENTIAL, KINETIC, E0)       0091

C     Communicating data to other nodes
      END
###################### md/src/MD_MASTER1_OMP.f ######################
!!
!! file for MD_MASTER1_OMP.f
!!
      SUBROUTINE MD_MASTER1_OMP(POTENTIAL, KINETIC, E0)
      REAL*8 POTENTIAL, KINETIC, E0
!$OMP master
      PRINT *, POTENTIAL, KINETIC, (POTENTIAL+KINETIC-E0)/E0            0004
!$OMP end master
      END
###################### md/src/STEP.f ######################


* Copyright 2007, 2008 Alain Muller, Frederique Silber-Chaussumier
*
*This file is part of STEP.
*
*The program is distributed under the terms of the GNU General Public
*License.




      subroutine STEP_get_myloopslice(i)
      INTEGER STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE
      COMMON /MYLOOPSLICE/ STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE

      i = STEP_MYLOOPSLICE
      end

      subroutine STEP_get_i_low(i)
      INTEGER STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE
      COMMON /MYLOOPSLICE/ STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE

      i = STEP_I_LOW
      END

      subroutine STEP_get_i_up(i)
      INTEGER STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE
      COMMON /MYLOOPSLICE/ STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE

      i = STEP_I_UP
      end

      subroutine STEP_Init()
      implicit none
      include 'STEP.h'
      integer*4 ierr,iBidon
      logical Initialized
      call MPI_Initialized(Initialized,ierr )
      if (.not. Initialized) then
         call MPI_Init ( ierr )
!     print *, STEP_size, STEP_rank
      end if
      call MPI_Comm_size ( MPI_COMM_WORLD, STEP_size, ierr )
      call MPI_Comm_rank ( MPI_COMM_WORLD, STEP_rank, ierr )      

      if (STEP_size .gt. STEP_MAX_NBNODE) then
         print *,"STEP_Init() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) STEP_size = ",STEP_size
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, ierr)
      end if
      end
      
      subroutine STEP_Finalize()
      implicit none
      include 'mpif.h'
      integer*4 ierr
      call MPI_Finalize(ierr)
      end

      subroutine STEP_Barrier()
      implicit none
      include 'mpif.h'
      integer*4 ierr
      call MPI_BARRIER(MPI_COMM_WORLD,ierr)
      end

      subroutine STEP_Get_size(size)
      implicit none
      include 'mpif.h'
      integer*4 size,ierr
      call MPI_Comm_size (  MPI_COMM_WORLD, size, ierr )
      end

      subroutine STEP_Get_rank(rank)
      implicit none
      include 'mpif.h'
      integer*4 rank,ierr
      call MPI_Comm_rank ( MPI_COMM_WORLD, rank, ierr )
      end
	
      function STEP_Get_thread_num()
      implicit none
      integer STEP_Get_thread_num
      call STEP_Get_rank(STEP_Get_thread_num)
      end

      subroutine STEP_ComputeLoopSlices(from,to,step,nb,size,bounds)
      implicit none
      integer F,T,from,to,step,nb,size
      parameter (F=1,T=2)
      integer bounds(F:T,size)
      integer nb_indices,nb_i,nb_e,i
      
c      print *,'from',from,'to',to,'step',step,'nb',nb,'size',size,
c     * 'bounds'

      do 5 i=1,size
         bounds(F,i)=-1
         bounds(T,i)=-1
 5    continue
      if (((step.GT.0) .AND. ((to-from).LT.step)) .OR.
     *     ((step.LT.0) .AND. ((from-to).LT.-step))) then ! pas d'iteration
c         print *,'pas d''iteration'
         do 10 i=1,nb
            bounds(F,i)=to
 10         bounds(T,i)=from
      else
         nb_indices=(to-from)/step+1
c         print *,nb_indices,' iterations'
         if (nb_indices.LE.nb) then ! une iteration par noeud
c            print *,'au plus une iteration par noeud'
            bounds(F,1)=from
            bounds(T,1)=from
            do 20 i=2,nb_indices
               bounds(F,i)= bounds(F,i-1)+step
 20            bounds(T,i)= bounds(F,i)
            do 25 i=nb_indices+1,nb ! plus d'iteration pour les autres noeuds
               bounds(F,i)= to
 25            bounds(T,i)= from
         else                   !au moins un noeud avec deux iterations
            nb_i=MOD(nb_indices,nb)
            nb_e=(nb_indices-nb_i)/nb
            if (nb_i.EQ.0) then
               nb_i=nb
               nb_e=nb_e-1
            endif
            bounds(F,1)=from
            bounds(T,1)=from+nb_e*step
            do 30 i=2,nb_i
c               print *, 'do30',i
               bounds(F,i)=bounds(T,i-1)+step
 30            bounds(T,i)=bounds(F,i)+nb_e*step
            do 35 i=nb_i+1,nb
c               print *,'do35',i
               bounds(F,i)=bounds(T,i-1)+step
 35            bounds(T,i)=bounds(F,i)+(nb_e-1)*step
         endif
      endif
    
c      print *,bounds
      end
      
******************************************
c
c     procédure produisant type_sub_region (handler de  type MPI) 
c     représentant la sous-region subregion définit dans l'espace d'indice region
c     d'élément de type type_region (handler de type MPI)
c
      subroutine type_subRegion(dim,region,sub_region,type_region,
     &     type_sub_region)
      implicit none
      include 'STEP.h'
      integer dim               !< nombre de dimension de l'espace
      integer L,U
      parameter (L=1, U=2)
      integer region(L:U,dim)      !< borne min/max des indices selon chaque dimension
      integer*4 sub_region(L:U,dim)      !< borne min/max des indices selon chaque dimension
      integer type_region       !< type MPI décrivant un element de la region
      integer type_sub_region   !> type MPI décrivant la sous-region
      
      integer*4 array_type(0:STEP_MAX_DIM),data_size(0:STEP_MAX_DIM),i
      integer*4 iBidon,IERROR
      logical valide

      if (dim .gt. STEP_MAX_DIM) then
         print *,"type_subRegion() : STEP_MAX_DIM 
     &trop petit (update STEP.h) dim = ",dim
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, IERROR)
      end if
!      print *,'dim=',dim,'region =',region,
!     &     'sub_region =',sub_region

      array_type(0)=type_region
      call MPI_TYPE_EXTENT(array_type(0),data_size(0),IERROR)
      i=1
      valide=.TRUE.
      do while (valide .AND. (i.LE.dim))
c     verification des définitions des régions/sous-regions
         if (.NOT.((region(L,i) .LE. sub_region(L,i)) .AND.
     &        (sub_region(L,i) .LE. sub_region(U,i)) .AND.
     &        (sub_region(U,i) .LE. region(U,i)))) then
            valide=.FALSE.
            array_type(dim)=MPI_DATATYPE_NULL
         else
            data_size(i)=data_size(i-1)*
     &           (region(U,i)-region(L,i)+1)
            call MPI_TYPE_HVECTOR(
     &           sub_region(U,i)-sub_region(L,i)+1, 1,
     &           data_size(i-1), array_type(i-1), array_type(i),IERROR)
         endif
         i=i+1
      enddo
      type_sub_region=array_type(dim)
      end
      
******************************************
c     
c     retourne la taille d'un tableau donc l'espace des indices 
c     est définit par region
      
      function STEP_SizeRegion(dim,region)
      implicit none
      integer dim,d,STEP_sizeRegion
      integer L,U
      parameter (L=1, U=2)
      integer region(L:U,dim)
      STEP_sizeRegion=1
      do 10 d=1,dim
         STEP_sizeRegion=STEP_sizeRegion*(region(U,d)-region(L,d)+1)
 10   continue
      end

c     
c     retourne l'indice dans un tableau linéaire dont l'indice commence à 1,
c     d'une case de cordonnées coords définit pour l'espace d'indice  region 
c     
      function indice(dim,coords,region)
      implicit none
      integer dim,d,indice
      integer coords(dim)
      integer L,U
      parameter (L=1, U=2)
      integer region(L:U,dim)
      indice = 0
      do 10 d=dim,1,-1
         indice=indice*(region(U,d)-region(L,d)+1)+
     &        coords(d)-region(L,d)
 10   continue
      indice=indice+1
      end
      
c     
c     retourne l'indice dans un tableau linéaire dont l'indice commence à 1,
c     de l'origine d'une sous-region subregion, définit dans l'espace d'indice
c     definit par region
c     
      function origine(dim,region,subregion)
      implicit none
      include 'STEP.h'
      integer dim,d,origine,indice
      integer L,U
      parameter (L=1,U=2)
      integer region(L:U,dim)
      integer subregion(L:U,dim)
      integer coords(STEP_MAX_DIM)
      integer*4 iBidon, IERROR

      if (dim .gt. STEP_MAX_DIM) then
         print *,"origine() : STEP_MAX_DIM 
     &trop petit (update STEP.h) dim = ",dim
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, IERROR)
      end if
      do 10 d=1,dim
         coords(d)=subregion(L,d)
 10   continue
      origine=indice(dim,coords,region)
!      print *,'\nregion=',region,'\nsubregion=',subregion,
!     &     '\ncoords=',coords,'\norigine=',origine
      end

******************************************
       subroutine STEP_WaitAll(NbReq,Request)
      implicit none
      include 'STEP.h'
      integer*4 NbReq,Request(NbReq)
      integer*4 Status(MPI_STATUS_SIZE,STEP_MAX_NBREQ)
      integer*4 iErr,iBidon

      if (NbReq .gt. STEP_MAX_NBREQ) then
         print *,"STEP_WaitAll() : STEP_MAX_NBREQ
     &trop petit (update STEP.h) NbReq = ",NbReq
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      call MPI_BARRIER(MPI_COMM_WORLD,iErr)
      if (iErr .ne. MPI_SUCCESS) then
         print *,'STEP_WaitAll() : MPI_BARRIER() Pb'
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if (NbReq .ne. 0) then
         call mpi_waitall(NbReq,Request,Status,iErr)
         if (iErr .ne. MPI_SUCCESS) then
            print *,'STEP_WaitAll() : mpi_wait_all() Pb'
            call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
         end if
      end if
      end

      subroutine AlltoAll_nBlocking_I1(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_I1(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      integer*1 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_I1(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      integer*1 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_INTEGER1,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_I1(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer*1 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_INTEGER1,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_I1(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      integer*1 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_INTEGER1,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_I1(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*1 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_I1() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER1,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_I1'
         call AlltoAll_nBlocking_I1(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_I1'
            call AlltoAll_even_I1(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_I1'
            call AlltoAll_odd_I1(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_I1'
            call AlltoAll_even2_I1(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_I1'
            call AlltoAll_odd2_I1(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_I1(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_I1(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*1 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_I1(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_I1(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end
      subroutine AlltoAll_nBlocking_I2(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_I2(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      integer*2 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_I2(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      integer*2 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_INTEGER2,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_I2(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer*2 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_INTEGER2,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_I2(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      integer*2 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_INTEGER2,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_I2(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*2 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_I2() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER2,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_I2'
         call AlltoAll_nBlocking_I2(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_I2'
            call AlltoAll_even_I2(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_I2'
            call AlltoAll_odd_I2(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_I2'
            call AlltoAll_even2_I2(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_I2'
            call AlltoAll_odd2_I2(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_I2(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_I2(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*2 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_I2(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_I2(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end
      subroutine AlltoAll_nBlocking_I4(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_I4(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      integer*4 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_I4(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      integer*4 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_INTEGER4,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_I4(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer*4 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_INTEGER4,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_I4(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      integer*4 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_INTEGER4,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_I4(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*4 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_I4() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER4,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_I4'
         call AlltoAll_nBlocking_I4(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_I4'
            call AlltoAll_even_I4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_I4'
            call AlltoAll_odd_I4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_I4'
            call AlltoAll_even2_I4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_I4'
            call AlltoAll_odd2_I4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_I4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_I4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*4 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_I4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_I4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end    
      subroutine AlltoAll_nBlocking_I8(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_I8(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      integer*8 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_I8(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      integer*8 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_INTEGER8,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_I8(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer*8 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_INTEGER8,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_I8(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      integer*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_INTEGER8,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_I8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_I8() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_I8'
         call AlltoAll_nBlocking_I8(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_I8'
            call AlltoAll_even_I8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_I8'
            call AlltoAll_odd_I8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_I8'
            call AlltoAll_even2_I8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_I8'
            call AlltoAll_odd2_I8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_I8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_I8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*8 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_I8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_I8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end
      subroutine AlltoAll_nBlocking_R4(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_R4(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      real*4 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_R4(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      real*4 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_REAL4,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_R4(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      real*4 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_REAL4,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_R4(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      real*4 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_REAL4,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_R4(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      real*4 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_R4() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_REAL4,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_R4'
         call AlltoAll_nBlocking_R4(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_R4'
            call AlltoAll_even_R4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_R4'
            call AlltoAll_odd_R4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_R4'
            call AlltoAll_even2_R4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_R4'
            call AlltoAll_odd2_R4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_R4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_R4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      real*4 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_R4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_R4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end          
      subroutine AlltoAll_nBlocking_R8(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_R8(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      real*8 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_R8(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      real*8 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_REAL8,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_R8(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      real*8 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_REAL8,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_R8(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      real*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_REAL8,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_R8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      real*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_R8() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_REAL8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_R8'
         call AlltoAll_nBlocking_R8(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_R8'
            call AlltoAll_even_R8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_R8'
            call AlltoAll_odd_R8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_R8'
            call AlltoAll_even2_R8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_R8'
            call AlltoAll_odd2_R8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_R8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_R8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      real*8 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_R8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_R8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end          
      subroutine AlltoAll_nBlocking_C8(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_C8(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      complex*8 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_C8(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      complex*8 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_COMPLEX8,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_C8(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      complex*8 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_COMPLEX8,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_C8(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      complex*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_COMPLEX8,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_C8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      complex*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_C8() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_COMPLEX8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_C8'
         call AlltoAll_nBlocking_C8(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_C8'
            call AlltoAll_even_C8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_C8'
            call AlltoAll_odd_C8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_C8'
            call AlltoAll_even2_C8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_C8'
            call AlltoAll_odd2_C8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_C8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_C8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      complex*8 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_C8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_C8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end    
      subroutine AlltoAll_nBlocking_C16(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_C16(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      complex*16 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_C16(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      complex*16 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_COMPLEX16,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_C16(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      complex*16 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_COMPLEX16,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_C16(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      complex*16 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_COMPLEX16,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_C16(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      complex*16 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_C16() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_COMPLEX16,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_C16'
         call AlltoAll_nBlocking_C16(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_C16'
            call AlltoAll_even_C16(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_C16'
            call AlltoAll_odd_C16(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_C16'
            call AlltoAll_even2_C16(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_C16'
            call AlltoAll_odd2_C16(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_C16(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_C16(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      complex*16 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_C16(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_C16(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end
      subroutine STEP_InitInterlaced_I1(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      integer*1 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_I1(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      integer*1 initial(1:size)   !tableau initial linéarisé
      integer*1 in(1:size)        !tableau in linéarisé
      integer*1 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_I1(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*1 array(1:size)     !tableau array linearise
      integer*1 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_I1() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER1,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_I1'
            call AlltoAll_even_Merge_I1(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_I1'
            call AlltoAll_odd_Merge_I1(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_I1'
            call AlltoAll_even2_Merge_I1(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_I1'
            call AlltoAll_odd2_Merge_I1(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_I1(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*1 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I1(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I1(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_I1(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*1 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*1 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I1(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I1(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_I1(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*1 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I1(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I1(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_I1(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*1 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I1(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I1(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end
      subroutine STEP_InitInterlaced_I2(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      integer*2 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_I2(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      integer*2 initial(1:size)   !tableau initial linéarisé
      integer*2 in(1:size)        !tableau in linéarisé
      integer*2 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_I2(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*2 array(1:size)     !tableau array linearise
      integer*2 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_I2() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER2,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_I2'
            call AlltoAll_even_Merge_I2(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_I2'
            call AlltoAll_odd_Merge_I2(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_I2'
            call AlltoAll_even2_Merge_I2(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_I2'
            call AlltoAll_odd2_Merge_I2(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_I2(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*2 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I2(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I2(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_I2(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*2 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*2 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I2(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I2(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_I2(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*2 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I2(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I2(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_I2(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*2 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I2(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I2(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end
      subroutine STEP_InitInterlaced_I4(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      integer*4 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_I4(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      integer*4 initial(1:size)   !tableau initial linéarisé
      integer*4 in(1:size)        !tableau in linéarisé
      integer*4 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_I4(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*4 array(1:size)     !tableau array linearise
      integer*4 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_I4() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER4,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_I4'
            call AlltoAll_even_Merge_I4(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_I4'
            call AlltoAll_odd_Merge_I4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_I4'
            call AlltoAll_even2_Merge_I4(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_I4'
            call AlltoAll_odd2_Merge_I4(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_I4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*4 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I4(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I4(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_I4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*4 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I4(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I4(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_I4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*4 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I4(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I4(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_I4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*4 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I4(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I4(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end    
      subroutine STEP_InitInterlaced_I8(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      integer*8 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_I8(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      integer*8 initial(1:size)   !tableau initial linéarisé
      integer*8 in(1:size)        !tableau in linéarisé
      integer*8 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_I8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*8 array(1:size)     !tableau array linearise
      integer*8 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_I8() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_I8'
            call AlltoAll_even_Merge_I8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_I8'
            call AlltoAll_odd_Merge_I8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_I8'
            call AlltoAll_even2_Merge_I8(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_I8'
            call AlltoAll_odd2_Merge_I8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_I8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*8 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I8(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I8(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_I8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*8 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_I8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_I8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end
      subroutine STEP_InitInterlaced_R4(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      real*4 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_R4(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      real*4 initial(1:size)   !tableau initial linéarisé
      real*4 in(1:size)        !tableau in linéarisé
      real*4 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_R4(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      real*4 array(1:size)     !tableau array linearise
      real*4 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_R4() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_REAL4,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_R4'
            call AlltoAll_even_Merge_R4(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_R4'
            call AlltoAll_odd_Merge_R4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_R4'
            call AlltoAll_even2_Merge_R4(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_R4'
            call AlltoAll_odd2_Merge_R4(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_R4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*4 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R4(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R4(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_R4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      real*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*4 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_R4(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_R4(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_R4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*4 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_R4(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_R4(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_R4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*4 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R4(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R4(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end          
      subroutine STEP_InitInterlaced_R8(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      real*8 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_R8(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      real*8 initial(1:size)   !tableau initial linéarisé
      real*8 in(1:size)        !tableau in linéarisé
      real*8 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_R8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      real*8 array(1:size)     !tableau array linearise
      real*8 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_R8() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_REAL8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_R8'
            call AlltoAll_even_Merge_R8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_R8'
            call AlltoAll_odd_Merge_R8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_R8'
            call AlltoAll_even2_Merge_R8(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_R8'
            call AlltoAll_odd2_Merge_R8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_R8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*8 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R8(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R8(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_R8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      real*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*8 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_R8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_R8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_R8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_R8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_R8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_R8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end          
      subroutine STEP_InitInterlaced_C8(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      complex*8 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_C8(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      complex*8 initial(1:size)   !tableau initial linéarisé
      complex*8 in(1:size)        !tableau in linéarisé
      complex*8 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_C8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      complex*8 array(1:size)     !tableau array linearise
      complex*8 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_C8() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_COMPLEX8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_C8'
            call AlltoAll_even_Merge_C8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_C8'
            call AlltoAll_odd_Merge_C8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_C8'
            call AlltoAll_even2_Merge_C8(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_C8'
            call AlltoAll_odd2_Merge_C8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_C8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*8 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C8(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C8(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_C8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      complex*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*8 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_C8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_C8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_C8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_C8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_C8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_C8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end    
      subroutine STEP_InitInterlaced_C16(size,array,array_initial,array_
     >buffer)
      implicit none
      integer size,i
      complex*16 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_C16(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      complex*16 initial(1:size)   !tableau initial linéarisé
      complex*16 in(1:size)        !tableau in linéarisé
      complex*16 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_C16(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      complex*16 array(1:size)     !tableau array linearise
      complex*16 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_C16() : STEP_MAX_NBN
     >ODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_COMPLEX16,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_C16'
            call AlltoAll_even_Merge_C16(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_C16'
            call AlltoAll_odd_Merge_C16(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_C16'
            call AlltoAll_even2_Merge_C16(STEP_Size,types,origin
     >es,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_C16'
            call AlltoAll_odd2_Merge_C16(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_C16(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*16 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C16(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C16(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_C16(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      complex*16 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*16 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_C16(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_C16(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_C16(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*16 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_C16(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_C16(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_C16(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*16 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C16(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C16(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end

###################### md/src/STEP.h ######################
* Copyright 2007, 2008 Alain Muller, Frederique Silber-Chaussumier
*
*This file is part of STEP.
*
*The program is distributed under the terms of the GNU General Public
*License.



      include 'mpif.h'

      integer STEP_MAX_NBNODE
      parameter (STEP_MAX_NBNODE = 16)

      integer MAX_NB_LOOPSLICES
      parameter (MAX_NB_LOOPSLICES = STEP_MAX_NBNODE)

      integer STEP_MAX_DIM
      parameter (STEP_MAX_DIM = 10)

      integer STEP_MAX_NBREQ
      parameter (STEP_MAX_NBREQ = 2*STEP_MAX_NBNODE*10)


      integer*4 STEP_size
      integer*4 STEP_rank
      COMMON /STEP/ STEP_size,STEP_rank

      integer STEP_SizeRegion

      integer*4 STEP_Status(1:MPI_STATUS_SIZE)
      integer*4 STEP_NbRequest

      integer IDX_SLICE_LOW,IDX_SLICE_UP,STEP_IDX
      parameter (IDX_SLICE_LOW = 1,IDX_SLICE_UP=2)

      integer STEP_NONBLOCKING,STEP_BLOCKING1,STEP_BLOCKING2
      parameter (STEP_NONBLOCKING=0,STEP_BLOCKING1=1,STEP_BLOCKING2=2)

      integer STEP_SUM,STEP_PROD
      integer STEP_AND,STEP_OR
      integer STEP_MIN,STEP_MAX
      integer STEP_IAND,STEP_IOR,STEP_IEOR
      parameter (STEP_SUM=MPI_SUM,STEP_PROD=MPI_PROD)
      parameter (STEP_AND=MPI_LAND,STEP_OR=MPI_LOR)
      parameter (STEP_MIN=MPI_MIN,STEP_MAX=MPI_MAX)
      parameter (STEP_IAND=MPI_BAND,STEP_IOR=MPI_BOR,STEP_IEOR=MPI_BXOR)
###################### md/src/UPDATE_PARDO1.f ######################
!!
!! file for UPDATE_PARDO1.f
!!
      SUBROUTINE UPDATE_PARDO1(I, I_L, I_U, J, ND, NP, POS, VEL, DT, 
     &A, F, RMASS)
      INTEGER I, I_L, I_U, J, ND, NP
      REAL*8 POS(1:ND, 1:NP), VEL(1:ND, 1:NP), DT, A(1:ND, 1:NP), F(1
     &:ND, 1:NP), RMASS
      DO I = I_L, I_U                                                   0006
         DO J = 1, ND                                                   0007
            POS(J,I) = POS(J,I)+VEL(J,I)*DT+0.5*DT*DT*A(J,I)            0008
            VEL(J,I) = VEL(J,I)+0.5*DT*(F(J,I)*RMASS+A(J,I))            0009
            A(J,I) = F(J,I)*RMASS                                       0010
         ENDDO
      ENDDO
      END
###################### md/src/UPDATE_PARDO1_MPI.f ######################
!!
!! file for UPDATE_PARDO1_MPI.f
!!
      SUBROUTINE UPDATE_PARDO1_MPI(I, I_L, I_U, J, ND, NP, POS, VEL, 
     &DT, A, F, RMASS)
      implicit none
      include "STEP.h"
      INTEGER*4 MAX_NB_REQUEST
      PARAMETER (MAX_NB_REQUEST = 2*3*MAX_NB_LOOPSLICES)
      INTEGER STEP_I_UP, STEP_I_LOW, I_UP, I_LOW, IDX, I, I_L, I_U, J
     &, ND, NP
      INTEGER STEP_SR_POS(IDX_SLICE_LOW:IDX_SLICE_UP, 1:2, 0:
     &MAX_NB_LOOPSLICES), STEP_SR_VEL(IDX_SLICE_LOW:IDX_SLICE_UP, 1:2
     &, 0:MAX_NB_LOOPSLICES), STEP_SR_A(IDX_SLICE_LOW:IDX_SLICE_UP, 1
     &:2, 0:MAX_NB_LOOPSLICES), STEP_I_LOOPSLICES(IDX_SLICE_LOW:
     &IDX_SLICE_UP, 1:MAX_NB_LOOPSLICES), STEP_REQUESTS(1:
     &MAX_NB_REQUEST)
      REAL*8 POS(1:ND, 1:NP), VEL(1:ND, 1:NP), DT, A(1:ND, 1:NP), F(1
     &:ND, 1:NP), RMASS

      CALL STEP_COMPUTELOOPSLICES(I_L, I_U, 1, STEP_SIZE,               0104
     &MAX_NB_LOOPSLICES, STEP_I_LOOPSLICES)                             0104

C     Put array boundaries into region arrays (SR: Send region)
C     First dimension: lower and upper bounds of each slice
C     Second dimension: for each dimension of the original array
C     Third dimention: store the boundaries of the local chunk. The first element stores initial boundaries, then one element for each process
      STEP_SR_A(IDX_SLICE_LOW,1,0) = 1                                  0111
      STEP_SR_A(IDX_SLICE_UP,1,0) = ND                                  0112
      STEP_SR_A(IDX_SLICE_LOW,2,0) = 1                                  0113
      STEP_SR_A(IDX_SLICE_UP,2,0) = NP                                  0114
      STEP_SR_VEL(IDX_SLICE_LOW,1,0) = 1                                0115
      STEP_SR_VEL(IDX_SLICE_UP,1,0) = ND                                0116
      STEP_SR_VEL(IDX_SLICE_LOW,2,0) = 1                                0117
      STEP_SR_VEL(IDX_SLICE_UP,2,0) = NP                                0118
      STEP_SR_POS(IDX_SLICE_LOW,1,0) = 1                                0119
      STEP_SR_POS(IDX_SLICE_UP,1,0) = ND                                0120
      STEP_SR_POS(IDX_SLICE_LOW,2,0) = 1                                0121
      STEP_SR_POS(IDX_SLICE_UP,2,0) = NP                                0122

C     Region computation
      DO 99999 IDX = 1, MAX_NB_LOOPSLICES                               0125
         I_LOW = STEP_I_LOOPSLICES(IDX_SLICE_LOW,IDX)                   0126
         I_UP = STEP_I_LOOPSLICES(IDX_SLICE_UP,IDX)                     0127
C  <A(PHI1,PHI2)-write-EXACT-{1<=PHI1, PHI1<=ND, PHI1<=3, I_LOW<=PHI2,
C    1<=PHI2, PHI2<=I_UP}>
         STEP_SR_A(IDX_SLICE_LOW,1,IDX) = 1                             0130
         STEP_SR_A(IDX_SLICE_UP,1,IDX) = MIN(ND, 3)                     0131
         STEP_SR_A(IDX_SLICE_LOW,2,IDX) = MAX(I_LOW, 1)                 0132
         STEP_SR_A(IDX_SLICE_UP,2,IDX) = I_UP                           0133
C  <VEL(PHI1,PHI2)-write-EXACT-{1<=PHI1, PHI1<=ND, PHI1<=3,
C    I_LOW<=PHI2, 1<=PHI2, PHI2<=I_UP}>
         STEP_SR_VEL(IDX_SLICE_LOW,1,IDX) = 1                           0136
         STEP_SR_VEL(IDX_SLICE_UP,1,IDX) = MIN(ND, 3)                   0137
         STEP_SR_VEL(IDX_SLICE_LOW,2,IDX) = MAX(I_LOW, 1)               0138
         STEP_SR_VEL(IDX_SLICE_UP,2,IDX) = I_UP                         0139
C  <POS(PHI1,PHI2)-write-EXACT-{1<=PHI1, PHI1<=ND, PHI1<=3,
C    I_LOW<=PHI2, 1<=PHI2, PHI2<=I_UP}>
         STEP_SR_POS(IDX_SLICE_LOW,1,IDX) = 1                           0142
         STEP_SR_POS(IDX_SLICE_UP,1,IDX) = MIN(ND, 3)                   0143
         STEP_SR_POS(IDX_SLICE_LOW,2,IDX) = MAX(I_LOW, 1)               0144
         STEP_SR_POS(IDX_SLICE_UP,2,IDX) = I_UP                         0145
99999    CONTINUE                                                       0146

C     Where work is done...
      STEP_I_LOW = STEP_I_LOOPSLICES(IDX_SLICE_LOW,STEP_RANK+1)         0149
      STEP_I_UP = STEP_I_LOOPSLICES(IDX_SLICE_UP,STEP_RANK+1)           0150
      CALL UPDATE_PARDO1(I, STEP_I_LOW, STEP_I_UP, J, ND, NP, POS,      0151
     &VEL, DT, A, F, RMASS)                                             0151

C     Communicating data to other nodes
C     3 communication shemes for all-to-all personalized broadcast :
C     STEP_NONBLOCKING, STEP_BLOCKING1 and STEP_BLOCKING2.
C     A nonblocking algo increment STEP_Nb_Request.
      STEP_NBREQUEST = 0                                                0158
      CALL STEP_ALLTOALLREGION_R8(2, STEP_SIZE, STEP_SR_A,              0159
     &STEP_SIZEREGION(2, STEP_SR_A(IDX_SLICE_LOW,1,0)), A, 0,           0159
     &MAX_NB_REQUEST, STEP_REQUESTS, STEP_NBREQUEST, STEP_NONBLOCKING   0159
     &)                                                                 0159
      CALL STEP_ALLTOALLREGION_R8(2, STEP_SIZE, STEP_SR_VEL,            0163
     &STEP_SIZEREGION(2, STEP_SR_VEL(IDX_SLICE_LOW,1,0)), VEL, 0,       0163
     &MAX_NB_REQUEST, STEP_REQUESTS, STEP_NBREQUEST, STEP_NONBLOCKING   0163
     &)                                                                 0163
      CALL STEP_ALLTOALLREGION_R8(2, STEP_SIZE, STEP_SR_POS,            0167
     &STEP_SIZEREGION(2, STEP_SR_POS(IDX_SLICE_LOW,1,0)), POS, 0,       0167
     &MAX_NB_REQUEST, STEP_REQUESTS, STEP_NBREQUEST, STEP_NONBLOCKING   0167
     &)                                                                 0167
C     If STEP_Nb_Request equals 0, STEP_WAITALL does nothing
      CALL STEP_WAITALL(STEP_NBREQUEST, STEP_REQUESTS)                  0172
      END
###################### md/src/UPDATE_PARDO1_OMP.f ######################
!!
!! file for UPDATE_PARDO1_OMP.f
!!
      SUBROUTINE UPDATE_PARDO1_OMP(I, I_L, I_U, J, ND, NP, POS, VEL, 
     &DT, A, F, RMASS)
      INTEGER I, I_L, I_U, J, ND, NP
      REAL*8 POS(1:ND, 1:NP), VEL(1:ND, 1:NP), DT, A(1:ND, 1:NP), F(1
     &:ND, 1:NP), RMASS
!$OMP parallel do private(J, I)
      DO I = I_L, I_U                                                   0007
         DO J = 1, ND                                                   0008
            POS(J,I) = POS(J,I)+VEL(J,I)*DT+0.5*DT*DT*A(J,I)            0009
            VEL(J,I) = VEL(J,I)+0.5*DT*(F(J,I)*RMASS+A(J,I))            0010
            A(J,I) = F(J,I)*RMASS                                       0011
         ENDDO
      ENDDO
!$OMP end parallel do
      END
###################### md/src/md.f ######################
!!
!! file for md.f
!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! This program implements a simple molecular dynamics simulation,
!   using the velocity Verlet time integration scheme. The particles
!   interact with a central pair potential.
!
! Author:   Bill Magro, Kuck and Associates, Inc. (KAI), 1998
!
! Parallelism is implemented via OpenMP directives.
! THIS PROGRAM USES THE FORTRAN90 RANDOM_NUMBER FUNCTION AND ARRAY 
!   SYNTAX
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

      PROGRAM MD
      implicit none

! simulation parameters
      
! dimensionality of the physical space
      INTEGER NDIM       
! number of particles
      INTEGER NPARTS     
! number of time steps in the simulation
      INTEGER NSTEPS     
      PARAMETER(NDIM=3,NPARTS=500,NSTEPS=500)
! mass of the particles
      REAL*8 MASS        
! time step
      REAL*8 DT          
! dimensions of the simulation box
      REAL*8 BOX(NDIM)   
      PARAMETER(MASS=1.0,DT=1.0E-4)

! simulation variables
      
      REAL*8 POSITION(NDIM,NPARTS)
      REAL*8 VELOCITY(NDIM,NPARTS)
      REAL*8 FORCE(NDIM,NPARTS)
      REAL*8 ACCEL(NDIM,NPARTS)
      REAL*8 POTENTIAL, KINETIC, E0
      INTEGER I , L
      CALL STEP_INIT

!!      box(1:ndim) = 10.
      DO L = 1, NDIM                                                    0044
         BOX(L) = 10.                                                   0045
      ENDDO

! set initial positions, velocities, and accelerations

      CALL INITIALIZE(NPARTS, NDIM, BOX, POSITION, VELOCITY, ACCEL)     0050

! compute the forces and energies

      CALL COMPUTE(NPARTS, NDIM, BOX, POSITION, VELOCITY, MASS, FORCE   0054
     &, POTENTIAL, KINETIC)                                             0054
      E0 = POTENTIAL+KINETIC                                            0056

! This is the main time stepping loop

      DO I = 1, NSTEPS                                                  0060
         CALL COMPUTE(NPARTS, NDIM, BOX, POSITION, VELOCITY, MASS,      0061
     &   FORCE, POTENTIAL, KINETIC)                                     0061
C     !$omp master
         CALL MD_MASTER1_MPI(POTENTIAL, KINETIC, E0)
C     !$omp end master
         CALL UPDATE(NPARTS, NDIM, POSITION, VELOCITY, FORCE, ACCEL,    0066
     &   MASS, DT)                                                      0066
      ENDDO
      CALL STEP_FINALIZE

      END

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! Compute the forces and energies, given positions, masses,
! and velocities
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
      SUBROUTINE COMPUTE(NP,ND,BOX,POS,VEL,MASS,F,POT,KIN)
      implicit none

      INTEGER NP
      INTEGER ND
      REAL*8  BOX(ND)
      REAL*8  POS(ND,NP)
      REAL*8  VEL(ND,NP)
      REAL*8  F(ND,NP)
      REAL*8  MASS
      REAL*8  POT
      REAL*8  KIN

      REAL*8 DOTR8
      EXTERNAL DOTR8
      REAL*8 V, DV, X

      INTEGER I, J, K, L
      REAL*8  RIJ(ND)
      REAL*8  D
      REAL*8  PI2
      PARAMETER(PI2=3.14159265D0/2.0D0)

! statement function for the pair potential and its derivative

! This potential is a harmonic well which smoothly saturates to a

! maximum value at PI/2.

C$PIPS STATEMENT FUNCTION V SUBSTITUTED
C$PIPS STATEMENT FUNCTION DV SUBSTITUTED

      POT = 0.0                                                         0039
      KIN = 0.0                                                         0040
C     !$omp parallel do default(shared) private(i,j,k,rij,d) reduction(+ : pot, kin)
      CALL COMPUTE_PARDO1_MPI(I, 1, NP, L, ND, NP, F, J, BOX, POS, 
     &RIJ, D, POT, PI2, K, KIN, VEL)
      KIN = KIN*0.5*MASS                                                0072

      END
       
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! Initialize the positions, velocities, and accelerations.
! The Fortran90 random_number function is used to choose positions.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
      SUBROUTINE INITIALIZE(NP,ND,BOX,POS,VEL,ACC)
      implicit none

      INTEGER NP
      INTEGER ND
      REAL*8  BOX(ND)
      REAL*8  POS(ND,NP)
      REAL*8  VEL(ND,NP)
      REAL*8  ACC(ND,NP)

      INTEGER I, J
      REAL*8 X

      DO I = 1, NP                                                      0020
         DO J = 1, ND                                                   0021
            CALL RANDOM_NUMBER(X)                                       0022
            POS(J,I) = BOX(J)*X                                         0023
            VEL(J,I) = 0.0                                              0024
            ACC(J,I) = 0.0                                              0025
         ENDDO
      ENDDO

      END

	SUBROUTINE   RANDOM_NUMBER(X)
	REAL*8 X
      X = 0.0                                                           0004
      END

! Compute the displacement vector (and its norm) between two particles.
      SUBROUTINE DIST(ND,BOX,R1,R2,DR,D)
      implicit none

      INTEGER ND
      REAL*8 BOX(ND)
      REAL*8 R1(ND)
      REAL*8 R2(ND)
      REAL*8 DR(ND)
      REAL*8 D

      INTEGER I

      D = 0.0                                                           0016
      DO I = 1, ND                                                      0017
         DR(I) = R1(I)-R2(I)                                            0018
         D = D+DR(I)**2.                                                0019
      ENDDO
      D = SQRT(D)                                                       0021

      END

! Return the dot product between two vectors of type real*8 and length n
      REAL*8 FUNCTION DOTR8(N,X,Y)
      implicit none

      INTEGER N
      REAL*8 X(N)
      REAL*8 Y(N)

      INTEGER I

      DOTR8 = 0.0                                                       0013
      DO I = 1, N                                                       0014
         DOTR8 = DOTR8+X(I)*Y(I)                                        0015
      ENDDO

      END

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! Perform the time integration, using a velocity Verlet algorithm
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
      SUBROUTINE UPDATE(NP,ND,POS,VEL,F,A,MASS,DT)
      implicit none

      INTEGER NP
      INTEGER ND
      REAL*8  POS(ND,NP)
      REAL*8  VEL(ND,NP)
      REAL*8  F(ND,NP)
      REAL*8  A(ND,NP)
      REAL*8  MASS
      REAL*8  DT

      INTEGER I, J
      REAL*8  RMASS

      RMASS = 1.0/MASS                                                  0021
C     !$omp parallel do default(shared) private(i,j)
      CALL UPDATE_PARDO1_MPI(I, 1, NP, J, ND, NP, POS, VEL, DT, A, F
     &, RMASS)

      END
