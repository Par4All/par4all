\documentstyle[12pt,A4,french,verbatim]{farticle}
\title{ Compilation pour machines a` me'moire re'partie \\
	 - Rapport de synthe`se finale - \\
Contrat 92/0017 BC 08}
\setlength{\parindent}{0mm}
\newcommand{\PIPS}{\mbox{PIPS}}
\newcommand{\CRI}{\mbox{CRI}}
\newcommand{\ARMINES}{\mbox{ARMINES}}
\newcommand{\IN}{\mbox{IN}}
\newcommand{\PRISM}{\mbox{PRISM}}
\newcommand{\OUT}{\mbox{OUT}}
\newcommand{\PVM}{\mbox{PVM}}
\author{Corinne Ancourt}
\begin{document}

\maketitle 

\section{Objet de l'e'tude}

L'objectif ge'ne'ral de cette e'tude est le de'veloppement de techniques
de compilation associe'es aux machines a` me'moire re'partie et leur
inte'gration dans une chai^ne de compilation ope'rationnelle.

Base'e sur une collaboration entre le \PRISM{} de l'Universite' de
Versailles-St Quentin et \ARMINES{}, la partie commune aux deux e'quipes
a pour but  la de'finition d'un format ge'ne'ral de directives
permettant l'inte'gration des re'sultats de'ja` fournis par l'un ou
l'autre des  outils de paralle'lisation automatique de programmes
scientifiques FORTRAN de'veloppe's par le \PRISM{} et \ARMINES{}.

La partie en charge d'\ARMINES{} repre'sente l'extension et la
consolidation des travaux mene's dans le cadre du projet ESPRIT PUMA.
Une maquette a e'te' de'veloppe'e. Elle 
ge'ne`re a` partir d'un programme FORTRAN 77 se'quentiel,  un programme FORTRAN 77 distribue' comportant des appels a` une
librairie de communications, permettant d'effectuer facilement des
tests sur un re'seau de stations de travail.

Plusieurs membres du CRI ont contribue' au bon de'roulement de ce
projet: Corinne Ancourt (Charge'e de recherche),  Fabien Coelho
(Scientifique du contingent en 94),  Be'atrice Creusillet (Ele`ve
chercheur), Franc,ois Irigoin (Directeur de recherche), et Ronan Keryell
(Charge' de recherche). 

\section{Inte're^t de l'e'tude}

Les multiprocesseurs a` me'moire distribue'e ont e'te' introduits sur le
marche' depuis de nombreuses anne'es mais leur programmation reste
ne'anmoins toujours difficile. Les donne'es d'une application doivent
e^tre divise'es et place'es sur les me'moires locales du
multiprocesseur. Les allocations des e'le'ments de tableaux ainsi
distribue's et des e'le'ments temporaires utilise's lors des calculs
doivent e^tre effectue'es. Des instructions {\em receive} doivent e^tre
ajoute'es a` chaque processus pour re'cupe'rer les donne'es re'sidant
dans d'autres processeurs et, de me^me, des instructions {\em send}
doivent e^tre introduites dans ceux des processus qui peuvent acce'der
localement ces me^mes e'le'ments.

Des outils de distribution automatique des donne'es d'une application
commencent a` faire leur apparition mais restent, pour le moment, dans
le domaine de la recherche \cite{Dart93,Feau93,GuPr93}.  Ecrire et
mettre au point des processus 
distribue's est beaucoup plus difficile que de de'clarer une boucle
paralle`le. Il faut tenir compte a` la fois de l'ordonnancement des
calculs et de la distribution des donne'es afin de minimiser les
communications et conserver un bon taux de paralle'lisme.

La majorite' des e'tudes de compilation associe'es aux multiprocesseurs
a` me'moire re'partie utilise la re`gle d'exe'cution, {\em owner
computes rule}.  Le code de transposition d'une matrice est simple, et
permet de montrer les limites de ce mod\`ele.

\begin{center}
\begin{figure}[htp]
\begin{verbatim}
DO I = 1, SIZE-1
   DO J = I+1, SIZE
      T = M(I,J)
      M(I,J) = M(J,I)
      M(J,I) = T
   ENDDO
ENDDO
\end{verbatim}
\label{code-transposition}
\caption{Transposition de matrice}
\end{figure}
\end{center}

\input{access-pattern.tex}

En effet, il est  difficile de distribuer les
it\'erations du corps de boucles de telle sorte qu'un processeur
poss\`ede tous les \'el\'ements qu'il doit mettre a` jour:
\verb+M(I,J)+ et \verb+M(J,I)+ sont dans la majorit\'e du temps sur des
processeurs diffe'rents.  La technique d\'efinie par Gerndt dans
\cite{Gerndt89} trouverait que l'ensemble de la matrice \verb+M+ doit
\^etre stock\'ee sur tous les processeurs (car elle correspond a` la
{\em zone de recouvrement} existant entre deux it\'erations
diff\'erentes). Il est de m\^eme impossible de recourir aux techniques
de {\em distribution} de boucles car la variable \verb+T+ est locale
au corps de boucles. Il faudrait
pour utiliser cette technique expanser la variable, ce qui reviendrait
a` allouer  une nouvelle matrice \verb+T+ aussi grande que la matrice
\verb+M+. 

 L'approche que nous avons choisie dans le cadre du projet PUMA (projet
ESPRIT 2701) est d'abandonner la {\em owner computes rule} et
de la remplacer par une me'moire partage'e e'mule'e par logiciel. Les
avantages potentiels sont:
\begin{enumerate}
\item la suppression du proble`me de la distribution des donne'es;
\item la prise en compte efficace de plus de constructions que ne le
permet la {\em owner computes rule};
\item la possibilite' de servir les reque^tes me'moire sur des
processeurs de'die's n'effectuant aucun calcul;
\item l'obtention plus aise'e d'un bon e'quilibre de charge puisque
n'importe quel processus peut e^tre exe'cute' sur n'importe quel
processeur;
\item le contro^le plus fin de la granularite' du paralle'lisme qui
n'est plus implicitement lie'e a` la distribution des donne'es; ceci
peut aussi ame'liorer l'e'quilibre de charge.
\end{enumerate}

La seconde ide'e est d'utiliser des serveurs me'moire spe'cifiques a`
l'application, connaissant les reque^tes me'moire devant e^tre
effectue'es au cours de l'exe'cution. Aucune reque^te me'moire explicite
n'est alors ne'cessaire, ce qui augmente la bande passante globale utile
du re'seau d'interconnexion.

Compte tenu des restrictions du type d'applications que nous
traitions dans le cadre du projet PUMA (nids de boucles paralle`les et
parfaitement imbrique'es), cette e'tude a permis l'extension du
prototype vers des applications petites ou moyennes beaucoup plus
ge'ne'rales, dont les caracte'ristiques sont de'crites dans la section
suivante. 

Les appels a` communication sont traduits en appels a` une librairie de
communications, \PVM{}, disponibles sur de nombreux multiprocesseurs.  Ce
choix  nous a permis d'effectuer des tests et de  commencer a` valider 
nos options  sur un re'seau de stations de travail.

\subsection{Objectifs pre'cis de la recherche}

L'objectif de cette e'tude est de de'velopper un prototype de compilation pour
machines a` me'moire distribue'e permettant de tester plusieurs
approches dans un contexte de {\em me'moire partage'e e'mule'e} sur diverses  machines a` me'moire re'partie posse'dant des
possibilite's de connexion processeur a` processeur: 
\begin{itemize}
\item diverses applications (diverses tailles de noyaux de calcul,
calculs creux), 
\item diverses distributions (dans un premier temps par colonne ou ligne)
\item divers  partitionnements (dans un premier temps selon la taille,
puis selon la forme)
\end{itemize}


 Le langage d'entre'e utilise' est le FORTRAN 77 et le langage de sortie
est le FORTRAN 77 augmente' d'appels a` une bibliothe`que de
communication portable \PVM{}.  Les points principaux de de'veloppement et
d'extension du langage e'taient les suivants:

\begin{itemize}
\item de'finition des pragmas;
\item de'veloppement de techniques de partitionnement pour des boucles non
parfaitement imbrique'es;
\item compilation de structures de contro^le dynamiques, comme les
conditionnelles;
\item compilation de structures de donne'es dynamiques, comme les
indirections;
\item de'veloppement de techniques d'analyse statique de complexite' en
temps.
\end{itemize}

Ces diffe'rents points ont e'te' traite's. Ils sont pre'sente's brie`vement en
Section \ref{etapes} et/ou  de'taille's en Section \ref {annexe}.
\subsection{Historique des e'tudes ante'rieures}

 De`s les anne'es 70, la DRET a soutenu un certain nombre de projets de
recherche en paralle'lisation. Le projet VESTA, de'veloppe' au sein du
Centre de Recherches de CII-Honeywell Bull avec la collaboration du
Pr.~Feautrier, pre'voit la conception d'un compilateur vectoriseur pour
Fortran. Ecrit en PL1, ce prototype n'a pas connu de suites imme'diates,
en partie du fait de l'absence de machines cibles franc,aises.
Le projet VATIL, de'veloppe' a` l'INRIA par l'e'quipe
du Pr.~Lichnewsky, a poursuivi dans cette voie de recherche par la
re'alisation d'un vectoriseur e'crit en Le-Lisp. Ce vectoriseur a e'te'
progressivement enrichi et transforme' en un paralle'liseur.

Le \CRI{}/\ARMINES{} a de'veloppe', avec un financement DRET, de 1988 a`
1991 le prototype \PIPS{} de paralle'lisation automatique de programmes
destine' aux machines a` me'moire partage'e (projet
DRET~87/017~bc~01). Les re'sultats les plus significatifs de cette
e'tude se re'sument par (1) la qualite' de la pre'cision des phases
d'analyse se'mantique utilise'es pour la de'tection automatique du
paralle'lisme implicite, (2) leur caracte`re inter-proce'dural ainsi que
(3) par la classe d'applications re'elles traite'es.

Cette e'tude s'est prolonge'e de 1992 a` 1993 par le contrat \PIPS{}-2
(DRET~87/017~bc~18) qui avait pour objectif l'e'valuation des travaux
pre'ce'demment effectue's et l'ajout d'une phase de ge'ne'ration de code
vectoriel approprie' au CRAY, incluant des primitives de
micro-tasking.

Paralle`lement a` ces deux projets (1991-1992), et sur un financement
ESPRIT, le \CRI{}/\ARMINES{} a de'bute' une e'tude de ge'ne'ration de code
distribue', de'die' aux machines a` me'moire re'partie posse'dant des
possibilite's de connexion processeur a` processeur. Cette e'tude a
permis le de'veloppement d'un prototype de compilation/distribution pour
des applications restreintes a` des nids de boucles paralle`les et
parfaitement imbrique'es.

\subsection{Re'sultats acquis ante'rieurement}

 \PIPS{} dispose d'un ensemble de phases
d'analyse (syntaxiques, se'mantiques,...), de transformations de
programme (distribution, paralle'lisation,...) et d'outils de
manipulation de structures de bases (polye`dres, graphes,...) qui, gr\^ace
a` sa structure modulaire, peuvent
e^tre utilise'es et applique'es inde'pendamment. Les de'veloppements
effectue's au cours de cette anne'e ont grandement profite' de cette
modularite'.

D'une part, les re'sultats des phases d'analyse des de'pendances, de
calcul des pre'conditions et de de'tection des {\em variables prive'es}
ont e'te' directement utilise'es. L'algorithme de distribution de nids
de boucles a e'te' ame'liore' pour mieux cibler les caracte'ristiques de
notre approche. L'algorithme utilise' par l'{\em atomizer} pour
transformer un programme en une suite d'affectations de variables
scalaires ou/et de re'fe'rences directes a` un tableau, a aussi e'te'
ame'liore'.

D'autre part, les phases de partitionnement de programmes, de
ge'ne'ration du code {\em distribu\'e/partitionne'}, de ge'ne'ration des
communications et des de'clarations associe'es aux allocations me'moire
ont e'te' re'alise'es ou adapte'es aux nouveaux besoins.  Ces phases
repre'sentent un module inde'pendant de \PIPS{} nomme'e {\em distribution}.
 


\section{De'roulement de l'e'tude}
\label{etapes}

Le projet {\em Compilation pour machines a` me'moire re'partie} s'est
de'roule' sur une pe'riode d'un an. Son objectif e'tait principalement
l'extension du langage d'entre'e restreint, dans le cadre du projet
ESPRIT PUMA, aux nids de boucles paralle`les et parfaitement
imbrique'es. Apre`s un rappel des caracte'ristiques de l'approche
retenue, et des diffe'rentes e'tapes utiles a` la ge'ne'ration d'un code
distribue' que nous avons e'tudie'es, les sections suivantes retracent les
e'tats d'avancement  du projet.

\subsection{Rappel des diffe'rentes e'tapes}

Avant de pre'senter les diffe'rentes e'tapes de cette e'tude, nous
rappelons les caracte'ristiques de l'approche retenue dans le cadre
de PUMA. 

\subsubsection{Caracte'ristiques de notre approche}

Les processus sont divise's en deux cate'gories. Les processus de
calculs exe'cutent les instructions du programme ainsi que les
communications ne'cessaires a` leur exe'cution. Seules les donne'es
utiles aux calculs associe's au processus seront alloue'es dans la 
me'moire locale du processeur associe'. Les processus qui e'mulent
la me'moire partage'e (bancs me'moire) n'effectuent que les
communications et le stockage.

\paragraph{\bf Distribution des donne'es}
La distribution des donn\'ees est implicite.  Les e'le'ments de tableaux
sont cycliquement allou\'es dans les bancs de la m\'emoire partag\'ee.
La figure 3 illustre cette distribution. Comme il s'agit de tableaux
ForTran, ils sont alloue's cycliquement par blocs d'e'le'ments
appartenant conse'cutivement a` une me^me colonne.  La taille des blocs
est donne'e par l'utilisateur dans le fichier de configuration
(model.rc) qui contient e'galement le nombre de processeurs et de bancs
me'moire.

\input{matrix-mapping.tex}

\paragraph{\bf Distribution des calculs}
La distribution des calculs est bas\'ee sur le partitionnement des
it\'erations des nids de boucles \cite{IrTr88}. Pour le moment, ce
partitionnement s'effectue par blocs r\'eguliers, parall\`eles aux
directions des boucles initiales \\(Figure \ref{control-distribution}). La taille des partitions peut \^etre
ais\'ement ajust\'ee de mani\`ere a` r\'eduire les surcou^ts de
communication et de contro^le du^s au d\'emarrage des ta^ches
parall\`eles, et a` e'quilibre les temps de communication et de calcul.
La granularit\'e du parall\'elisme peut \^etre aussi ajust\'ee, car elle
n'est pas directement li\'ee a` la distribution des donn\'ees mais
uniquement au graphe de contr\^ole du programme.

\begin{figure}
\setlength{\unitlength}{1.0pt}
\begin{picture}(180,90)(0,0)

% partitioning

\thicklines
\put(0,90){\line(1,0){90}}
\put(30,60){\line(1,0){90}}
\put(90,30){\line(1,0){60}}
\put(150,0){\line(1,0){30}}

\put(0,90){\line(1,-1){30}}
\put(30,90){\line(1,-1){60}}
\put(60,90){\line(1,-1){90}}
\put(90,90){\line(1,-1){90}}

% iteration domain

\thinlines
\put(10,85){\line(1,0){70}}
\put(10,85){\line(2,-1){140}}
\put(80,85){\line(1,-1){70}}

% iterations

\multiput(10,85)(10,0){8}{\circle{2}}
\multiput(30,75)(10,0){7}{\circle{2}}
\multiput(50,65)(10,0){6}{\circle{2}}
\multiput(70,55)(10,0){5}{\circle{2}}
\multiput(90,45)(10,0){4}{\circle{2}}
\multiput(110,35)(10,0){3}{\circle{2}}
\multiput(130,25)(10,0){2}{\circle{2}}
\multiput(150,15)(10,0){1}{\circle{2}}

\end{picture}

\caption{Distribution des calculs}
\label{control-distribution}
\end{figure}

Les d\'ependences entre
instructions sont analys\'ees de mani\`ere a` trouver les
contraintes d'ordonnancement entre les it\'erations des boucles.  Si cet
ordonnancement est un ordre total, les boucles devront s'ex\'ecuter
s\'equentiellement et il ne sera pas possible d'effectuer un
partitionnement int\'eressant des instructions permettant de r\'eduire
le temps d'ex\'ecution.  Des techniques de partitionnement peuvent
toutefois \^etre utilis\'ees pour pallier aux contraintes de taille des
m\'emoires locales (r\'eduction des donn\'ees devant \^etre stock\'ees
au cours de l'ex\'ecution).

Chaque bloc est vu comme une ta^che ind\'ependante. Chaque ta^che
est constitu\'ee de trois parties: la premi\`ere correspond aux
transferts des donn\'ees utiles a` l'ex\'ecution de la ta^che de la
m\'emoire globale vers la m\'emoire locale du processeur, la seconde
correspond aux calculs et la derni\`ere aux transferts des donn\'ees,
modifi\'ees au cours de l'ex\'ecution, dans la m\'emoire partag\'ee.

Les ta^ches paralle`les sont distribue'es sur les processeurs
 de calcul. Les calculs ne font plus re'fe'rence aux e'le'ments de tableaux
via les indices de  boucles initiaux, mais utilisent les indices locaux a` 
chacun des blocs (partitions). Tous les calculs qui de'pendent des
indices de boucles initiaux sont remplace's par une se'rie
d'instructions relatives aux indices locaux.

D'une part, pour respecter le mode`le SPMD et, d'autre part, pour limiter les
synchronisations et communications e'ventuelles internes a` un nid de
boucles, l'ensemble des exe'cutions se'quentielles est effectue' par tous
les processeurs. Ce choix limite les diffusions de donne'es qui seraient
calcule'es se'quentiellement et utilise'es au cours de calculs
paralle`les distribue's internes au nid de boucles se'quentiel.

Un bon \'equilibre de charge est plus facile a` obtenir que dans le cas
de la {\em owner computes rule} car n'importe quel processus
peut \^etre ex\'ecut\'e sur n'importe quel processeur. La taille des
processus peut m\^eme \^etre ajust\'ee dynamiquement.

\paragraph{\bf Transferts de donn\'ees}

Lorsque le partitionnement des instructions en processus
est d\'etermin\'e, les transferts de donn\'ees entre la m\'emoire 
partag\'ee \'emul\'ee et les m\'emoires locales des processeurs de calcul 
peuvent \^etre calcul\'es.

Les transferts, de la m\'emoire partag\'ee vers les m\'emoires locales des
processeurs de calcul, des donn\'ees utilis\'ees au cours de l'ex\'ecution
doivent \^etre ex\'ecut\'es avant le d\'ebut de chaque processus.  Ceux de
la m\'emoire locale des processeurs de calcul vers la m\'emoire partag\'ee
doivent l'\^etre a` la fin de chaque ex\'ecution.  Ils n\'ecessitent la
caract\'erisation de l'ensemble des donn\'ees {\em utilis\'ees} et {\it
modifi\'ees} pendant les phases de calculs.

La caract\'erisation de ces donn\'ees est probl\'ematique quand il
s'agit de tableaux, car c'est l'ensemble des \'el\'ements
r\'ef\'erenc\'es par le tableau dans le corps de boucle de la ta^che
parall\`ele qui doit \^etre calcul\'e. Cet ensemble correspond a` un
ensemble de points entiers.  C'est la cause de la plupart des
probl\`emes rencontr\'es.  Cet ensemble ne correspond pas toujours a` un
ensemble convexe de points entiers ce qui rend difficile son parcours
par un corps de boucles {\em simples}. Les techniques utilis\'ees pour
g\'en\'erer automatiquement ces codes de transfert sont d\'etaill\'ees
dans~\cite{Ancourt90}.

\paragraph{\bf Chargement des donn\'ees.}
Pour transf\'erer les donn\'ees {\em utilis\'ees} de la m\'emoire partag\'ee vers 
les m\'emoires locales, il n'est pas n\'ecessaire de copier l'ensemble exact des 
donn\'ees utilis\'ees par la ta^che. Un ensemble de donn\'ees l\'eg\`erement 
plus grand peut \^etre copi\'e car ces donn\'ees ne seront utilis\'ees que 
localement. Aucun conflit entre les diff\'erentes m\'emoires locales ne peut se 
produire. 

Si l'ensemble des \'el\'ements a` transf\'erer est non convexe, son
enveloppe convexe sera transf\'er\'ee.  Dans le cas de plusieurs
r\'ef\'erences au m\^eme tableau en d\'ependance dans le corps de boucles, un
seul code de transfert des donn\'ees sera g\'en\'er\'e pour l'ensemble de
ces r\'ef\'erences.

\paragraph{\bf Rangement des r\'esultats.}
Pour transf\'erer les donn\'ees de la m\'emoire locale des processeurs de
calcul vers la m\'emoire partag\'ee, il ne serait pas correct de copier un
\'el\'ement qui n'a pas \'et\'e modifi\'e par la ta^che. En effet, cet
\'el\'ement peut avoir \'et\'e modifi\'e par une ta^che s'ex\'ecutant en
parall\`ele, et deux valeurs d'une m\^eme donn\'ee ne peuvent coexister
dans une m\'emoire partag\'ee coh\'erente. Il faut donc recopier l'ensemble
exact des donn\'ees {\em modifi\'ees} par la ta^che.

\input{bank-access-pattern.tex}

Si cet ensemble n'est pas un ensemble convexe, des divisions enti\`eres sont
introduites dans les expressions des bornes de boucles de mani\`ere a`
traduire cette non convexit\'e. Un test lin\'eaire est aussi parfois
n\'ecessaire. Pour \'eviter ce test, qui conduit a` une augmentation du
cou^t du contr\^ole, les codes de transfert pourraient \^etre de'compose's:
un code de transfert par ensemble convexe constituant l'ensemble non
convexe des donn\'ees modifi\'ees serait alors g\'en\'er\'e.


La figure \ref{data-movements} illustre l'ensemble des e'le'ments
re'fe'rence's par l'un des blocs de calculs lors d'une transposition de
matrice. L'ensemble des e'le'ments du bloc infe'rieur gauche et
supe'rieur droit doivent e^tre transfe're's pour mettre a` jour les
e'le'ments de tableaux syme'triques correspondants. La distribution de
ces e'le'ments re'fe'rence's sur les bancs de la me'moire partage'e
illustre bien la non convexite' des ensembles d'e'le'ments a`
transfe'rer. Cette non convexite' explique  la complexite' du
code ge'ne're'. 


\paragraph{\bf De'claration des variables locales et partage'es}
En ce qui concerne les proble`mes d'allocation me'moire et de
de'clarations des donne'es partage'es et locales associe'es, on
remarque que : 
\begin{itemize}
\item Il est important de pouvoir d\'etecter les variables locales a` un
corps de boucles, telle que $T$ dans l'exemple de la transposition de
matrice, parce qu'elles peuvent \^etre allou\'ees directement dans les
m\'emoires locales des processeurs de calcul. Elles n'ont pas besoin
d'\^etre transf\'er\'ees entre la m\'emoire partag\'ee et les m\'emoires
locales durant les ex\'ecutions.  Les variables scalaires {\it
priv\'ees} sont d\'etect\'ees par \PIPS{}\cite{IrJoTr91}. Elles sont
d\'eclar\'ees dans les m\'emoires locales mais pas dans la m\'emoire
partag\'ee \'emul\'ee.

\item Il est aussi indispensable de prendre en compte les {\em input
dependences} qui existent entre des lectures de la m\^eme variable pour
savoir combien de copies locales il faut associer a` chaque variable initiale.

Dans le cas de la transposition de matrice, les deux r\'ef\'erences
\verb+M(I,J)+ et \verb+M(J,I)+ sont totalement ind\'ependantes. Il faut donc
pr\'evoir la d\'eclaration de {\em deux} zones de copies locales pour
\verb+M+, qui sont appel\'ees \verb+L_M_0+ et \verb+L_M_1+(\verb+L+ comme
local). 
Par contre, dans le cas d'un calcul de convolution, on trouverait que
toutes les r\'ef\'erences a` un voisinage sont en d\'ependance et donc
qu'il ne faut allouer qu'une seule copie locale.
\end{itemize}

\subsubsection{Premier trimestre}

Les de'veloppements re'alise's au cours du premier trimestre avaient
pour but essentiel d'installer une proce'dure de validation comple`te
des codes distribue's ge'ne're's par notre prototype. Ces de'veloppements
se sont concre'tise's par  

\begin{itemize}

\item l'installation d'une proce'dure de validation du code ge'ne're' 
 qui comporte des  programmes contenant, actuellement, en fin de projet:
\begin{itemize}
\item plusieurs nids de boucles parfaitement et non parfaitement
imbrique's, se'quentiels, partiellement ou totalement paralle`les
englobant une se'quence d'affectations,
\item d'e'ventuelles indirections,
\item mais pas de conditionnelles,
\item  des re'fe'rences a` des e'le'ments de tableaux qui  sont  disjointes ou  se recouvrent.
\end{itemize}

\item l'ajout d'une interface avec \PVM{} et de sa validation sur re'seau de stations de
travail,   pre'sente'es dans
le rapport interme'diaire.

\end{itemize} 

\paragraph{\bf Les indirections}

Simultane'ment, nous avons commence' l'extension du langage d'entre'e
conside're'. 
La pre'sence des indirections dans les programmes soule`vent les me^mes
proble`mes que ceux expose's pour les affectations conditionnelles: on
ne connai^t pas a` la compilation les e'le'ments re'fe'rence's au cours
des calculs. 

 L'{\em atomizer} est une transformation de programme qui simplifie les
phases d'analyse et de ge'ne'ration des communications dans les cas ou`
il y a des indirections (et des entre'es/sorties), puisque les variables
temporaires cre'e'es par l'atomizer repre'sentent les diffe'rentes
ope'rations ou communications qui devront e^tre effectue'es pour
finaliser l'envoi ou la re'ception d'un acce`s indirect aux e'le'ments
d'un tableau.

\begin{center}
\begin{figure}[hpt]
\begin{verbatim}
                                      ITMP1 = I             
                                      ITMP2 = J
                                      ITMP3 = A(ITMP1, ITMP2)
                                      ITMP4 = ITMP1 -1
                                      ITMP5= ITMP2 +1
       B(I-1,J+1)=A(I,J)              B(ITMP4,ITMP5) = ITMP3
       C(A(I,J),J) = I                C(ITMP3,ITMP2) = ITMP1
\end{verbatim}
\caption{Atomizer}
\label{prog3}
\end{figure}
\end{center}

Cette transformation de programme a e'te' inte'gre'e a` \PIPS{}. Si
l'optimiseur d'un compilateur sait optimiser le code ge'ne're',
notre prototype n'effectue pas d'optimisations. Afin de minimiser les
allocations de temporaires, une passe simplifie'e de cette
transformation, permettant de pre'ciser que l'on ne de'sire appliquer la
transformation qu'aux re'fe'rences aux e'le'ments de tableaux
indirectes, a aussi e'te' inte'gre'e a` \PIPS.


\subsubsection{Deuxie`me trimestre}
Cette deuxie`me partie du projet  a e'te' consacre'e, d'une part, a` la
de'finition des pragmas et, d'autre part, a` la de'finition du sche'ma
de calcul et de 
communications adapte' aux nouvelles structures de donne'es et de
contro^le introduites.


\paragraph{\bf De'finition des pragmas.} En collaboration avec le
\PRISM{}, nous avons
de'termine' les informations inte'ressantes qui seront e'change'es entre
PAF et PIPS. Plus pre'cise'ment, il s'agit des {\em pre'conditions} et
des {\it re'gions}, calcule'es par PIPS et disponibles sous forme de
commentaire dans les programmes, qui seront utilise'es par PAF et des
{\it re'ductions}, de'tecte'es par PAF, qui seront utilise'es par
PIPS. Le format d'e'changes des re'ductions choisi est celui de'crit
dans le premier rapport d'avancement re'dige' par M. Barreteau,
P. Feautrier et X. Redon.


\paragraph{\bf Sche'ma de calcul et communications adapte' aux nouvelles structures.}
Dans le cadre du projet PUMA, seule la ge'ne'ration de code distribue'
pour les nids de  boucles parfaitement imbrique'es et paralle`les e'tait
propose'e et optimise'e. L'extension du type d'applications traite'es
ne'cessitait la de'finition d'un nouveau sche'ma de calcul et de
communications. Une e'bauche de cette e'tude a e'te' pre'sente'e dans
le rapport interme'diaire. Nous en donnons une version  plus
de'taille'e dans l'annexe~\ref{annexe}.
 
\paragraph{\bf Complexite' statique}

Le calcul de la complexite' des ta^ches paralle`les  a` exe'cuter sur
les diffe'rents processeurs est utile, en autres, pour e'valuer  le bon 
e'quilibrage ``cou^ts des communications'' et ``cou^ts des calculs''.

Cette phase de calcul, qui est inte'gre'e a` \PIPS{}, a e'te'
pre'sente'e brie`vement dans le rapport interme'diaire et de'taille'e
dans~\cite{Zhou94}. Pour des raisons de temps (re'duction de la dure'e
du projet de 3 a` 1 an), les re'sultats de cette phase d'analyse ne sont
pas encore exploite's par l'algorithme de partitionnement. Cette
optimisation fait, cependant, partie des perspectives inte'ressantes de
recherche et d'expe'rimentation.



\subsubsection{Troisie`me trimestre}

Cette pe'riode a e'te' consacre'e a` l'implantation du sche'ma de calcul
et communications, pre'sente' en section~\ref{annexe} , a` l'implantation 
et  l'ame'lioration des re'gions \IN{} et \OUT{}. 

\paragraph{\bf Re'gions \IN{} et \OUT{}}

Les techniques de compilation, utilise'es actuellement par le prototype,
traitent successivement les nids de boucles. Elles ge'ne`rent pour
chacun d'entre eux, une ta^che paralle`le compose'e des communications
de la me'moire partage'e vers les me'moires locales, du code de calcul,
puis des communications des me'moires locales vers la me'moire
partage'e. Etant donne' que les parties de programme se'quentielles sont
exe'cute'es par l'ensemble des processeurs, lorsque plusieurs nids de
boucles successifs totalement se'quentiels re'fe'rencent le me^me
ensemble de donne'es, ces techniques ge'ne'rent des communications
inutiles qui ne tiennent pas compte de l'he'ritage de donne'es
cohe'rentes provenant de ta^ches se'quentielles pre'ce'dentes. Pour
palier ce surcou^t de communications, il ne faut effectuer (si les
capacite's me'moire le permettent) les communications qu'au de'but et a`
la fin de la partie se'quentielle du programme. L'ensemble des donne'es
a` transfe'rer est, dans ce cas, donne' par les re'gions \IN{} et \OUT{}
\cite{Creu95} de la section de programme conside're'e. 



\subsubsection{Dernier trimestre}


La dernie`re partie du projet a permis de re'aliser une phase de
transformation adapte'e aux nids de boucles mal imbrique's et
d'ame'liorer les de'clarations associe'es aux allocations me'moire,
aussi bien pour les processeurs de calcul que pour les processeurs
me'moire. 


\paragraph{\bf De'claration des tableaux locaux}

Les processeurs de calcul exe'cutent les parties de calcul correspondant
aux ite'rations se'quentielles et aux ite'rations paralle`les
qui leur sont attribue'es. L'espace ne'cessaire au stockage des e'le'ments
de tableaux re'fe'rence's au cours de ces ite'rations doit e^tre alloue'
en {\em local}.

Plusieurs re'fe'rences a` un me^me tableau caracte'risant des sections
de tableau disjointes sont alloue'es de manie`re inde'pendante.  Comme
l'illustre le code de la transposition de matrice en Annexe, les
sections syme'triques du tableau \verb+M+ : \verb+L_M_0+ et \verb+L_M_1+
(Local variable \verb+M+)
re'fe'rence'es au sein du me^me nid de boucles paralle`les sont
alloue'es inde'pendamment .  Si les re'fe'rences caracte'risent des
sections non disjointes, un me^me espace leur est alloue'. Sa dimension
correspond a` l'enveloppe convexe des diffe'rentes sections.  L'espace
alloue' aux tableaux locaux de'pend directement de la taille des blocs
de'finie par le partitionnement.


\paragraph{\bf De'claration des variables globales}

Les processeurs me'moire e'mulent une me'moire globale sur laquelle est
alloue' l'ensemble des tableaux de l'application. Les tableaux sont
stocke's cycliquement sur les bancs (processeurs me'moires) par blocs,
de taille e'gale a` celle des lignes de bancs de'finies dans le fichier
de configuration.  La position d'un e'le'ment de tableau \verb+M+ est
de'finie par le nume'ro de banc auquel il appartient, le nume'ro de la
ligne sur le banc et sa position dans la ligne.  La taille des tableaux
en me'moire globale est identique a` celle des tableaux initiaux.  Une
matrice  \verb+M(100x100)+ distribue'e sur 4 bancs me'moire par lignes de 100
e'le'ments sera de'clare'e sur chacun des bancs sous la forme
\verb+ES_M(0:24,0:99)+ (Emulated Shared variable  \verb+M+).

\paragraph{\bf  Traitement des nids de boucles mal imbrique'es}

Afin d'augmenter le taux de paralle'lisme exploitable et les
possibilite's d'appliquer efficacement le partitionnement, le code est
tout d'abord transforme'~; une distribution partielle est
applique'e. Apre`s distribution en composantes fortement connexes des
diffe'rentes instructions d'un nid de boucles, les instructions
inde'pendantes (non lie'es par des de'pendances) sont regroupe'es au
sein d'un me^me nid de boucles qui sera au moins partiellement (pour
l'indice de boucle conside're') paralle'lise'. Pluto^t qu'une
distribution totale, cette transformation permet, dans le cadre de notre
e'tude, de diminuer l'overhead de contro^le et d'ame'liorer
l'e'quilibrage de charge.  Les instructions lie'es par des de'pendances
cycliques, qui ne peuvent pas e^tre {\em casse'es} par simple
distribution des instructions, sont aussi regroupe'es au sein d'un me^me
nid de boucles, dans le me^me but.  Les instructions qui appartiennent
a` une me^me composante connexe sont effectivement distribue'es de
fac,on a` permettre la paralle'lisation.

L'application de cette transformation ge'ne`re des nids de boucles
parfaitement bien imbrique'es paralle`les ou se'quentiels et des nids de
boucles mal imbrique's dont la boucle externe  est
se'quentielle. 

\subsection{Difficulte's, faits significatifs et re'sultats}

La re'alisation de cette e'tude n'a pas souleve' de proble`me technique
particulier. La difficulte' que nous avons rencontre' est temporelle et
s'explique par la re'duction de trois a` un an de la pe'riode alloue'e
initialement au projet . Les diffe'rentes e'tapes du projet ont e'te'
aborde'es et la majorite' d'entre elles a e'te' implante'e dans le
prototype de paralle'lisation \PIPS{}.


Avec ces nouvelles phases d'analyse, de transformation et
ge'ne'ration de code distribue', \PIPS{} dispose d'une plateforme de
recherche et d'expe'rimentation, tre`s modulaire, qui devrait permettre
d'effectuer des comparaisons inte'ressantes avec les approches base'es
sur la  {\em owner computes rule} et plus particulie`rement l'approche
\verb+HPF+ . 

Cette e'tude a permis l'introduction de nouvelles phases inde'pendantes
dans \PIPS{} qui pourront e^tre utilise'es dans un autre contexte. 
Notamment, la transformation de distribution partielle sera utile a`
tout type d'architecture ou strate'gie de compilation ne'cessitant un
grain moyen de paralle'lisme.


\section{Re'capitulation des re'sultats}

Le de'veloppement de ce prototype a montre' que PIPS posse'dait les
phases d'analyses et de transformations ne'cessaires au de'veloppement
rapide d'algorithmes de ge'ne'ration de code distribue'. Les e'tapes
 inte'gre'es a` \PIPS{} sont les suivantes:
\begin{itemize}
\item de'finition des pragmas permettant aux deux e'quipes \PRISM{} et
\CRI{} de profiter rapidement des re'sultats des techniques avance'es
de'veloppe'es par l'un et  l'autre des prototypes,
\item compilation des structures de contro^le se'quentielles,
\item compilation des structures de donne'es dynamiques, comme les
indirections
\item de'veloppement de techniques de partitionnement pour des boucles non
parfaitement imbrique'es
\item ge'ne'ration de code faisant appel a` une bibliothe`que de
communications \PVM{},
\item de'veloppement d'une phase de validation permettant de conserver
au cours  des ame'liorations des algorithmes de ge'ne'ration de code, les re'sultats ante'rieurs acquis sans
possibilite's de re'gression. 
\end{itemize}

Les phases qu'il reste a` inte'grer en vue d'optimiser le code ge'ne're'
sont:

\begin{itemize}
\item compilation de structures de contro^le dynamiques, comme les
conditionnelles 
\item inte'gration des techniques d'analyse statique de complexite' en
temps 
\item inte'gration des re'gions \IN{} et \OUT{}
\end{itemize}


\section{Conclusion}

Apre`s quelques mois de retard, du^ en grande partie au fait que l'on
de'sirait, malgre' la re'duction de temps imparti au projet, de'velopper
et inte'grer la majeure partie des phases de l'e'tude dans la chaine de
compilation ope'rationelle \PIPS{}, en vue d'obtenir une maquette du
prototype de ge'ne'ration de code distribue' avance'e, ce projet s'est
bien de'roule' et a fournit un ensemble de  re'sultats techniques
cohe'rents en un an.


\section{Perspectives ulte'rieures}

Comme nous l'avons de'ja` signale' au cours de la pre'sentation des
diffe'rentes e'tapes de re'alisation de cette e'tude, par manque de
temps, certaines optimisations ne sont pas inte'gre'es dans les e'tapes
de ge'ne'ration de code distribue'. La phase de calcul de la complexite'
statique d'une application devrait permettre d'ame'liorer le {\em load
balancing} des applications paralle`les (partitionnement selon une
dimension pluto^t qu'une autre). La phase d'analyse des {\em re'gions
\IN{} et \OUT{}} devrait aussi, une fois inte'gre'e aux algorithmes de
ge'ne'ration des comunications, conduire a` une diminution significative
du cou^t des communications.

Une autre perspective importante est l'utilisation des diffe'rentes
phases de {\em distribution} pour valider et tester les choix retenus
dans notre approche. Les premiers re'sultats expe'rimentaux, mene's sur
un re'seau de stations de travail, sont ceux que l'on attendait. Notre
approche offre des possibilite's d'exe'cution d'applications gourmandes
en me'moire et (2) obtiennent de meilleurs temps d'exe'cution sur les
ta^ches paralle`les comportant de nombreux calculs.

\section{Annexe Technique - Sche'ma de communication adapte' aux nouvelles
structures}
\label{annexe} 

La premie`re e'tape de distribution/paralle'lisation d'un programme,
compose' uniquement d'un ensemble de boucles paralle`les et parfaitement
imbrique'es, consiste a` regrouper les calculs en ta^ches paralle`les de
taille raisonnable (relativement a` la taille des caches ou aux lignes de
bancs me'moire, par exemple). Une fois les calculs regroupe's en ta^ches
paralle`les, l'e'tape suivante est la ge'ne'ration des communications
ne'cessaires a` l'exe'cution des ta^ches sur les diffe'rents
processeurs. Pour ce type de code, la strate'gie choisie consistait a`
transfe'rer l'ensemble des e'le'ments re'fe'rence's en lecture
(respectivement en e'criture) avant (respectivement apre`s) l'exe'cution
de chacune des ta^ches paralle`les. Cependant, l'extension de notre
langage d'entre'e impose un sche'ma de communication plus complet.

\paragraph{\bf Recouvrement des calculs et communications}

Les expe'riences re'alise'es ont e'te' effectue'es sur un re'seau de
stations de travail. Le gain que l'on peut attendre de ce type
d'expe'rience se situe sur la granularite' des applications qui peuvent
e^tre exe'cute'es compare'es a` une exe'cution sur un seul
processeur. Dans ce contexte, un pipeline de se'quences d'instructions
effectuant les communications et d'autres les calculs n'a pas
d'inte're^t.  En fait, il induirait me^me un surcroi^t d'allocations
me'moire, ne'cessaire au stockage des tableaux locaux pour les
diffe'rents niveaux de pipeline. Nous n'avons donc pas implante' de
techniques permettant de recouvrir calculs et communications, et
l'ensemble des communications ne'cessaires aux calculs d'une ta^che sont
effectue'es imme'diatement avant l'exe'cution des calculs.


     
\subsection{Strate'gies de communication} 

Diffe'rentes strate'gies de communication peuvent e^tre utilise'es pour
transfe'rer les e'le'ments utiles a` l'exe'cution des ta^ches sur les
processeurs selon les diffe'rentes structures du programme.  Celle que
nous avons choisie est la suivante:

\subsubsection{Affectation simple}
 \begin{itemize}
\item les e'le'ments   re'fe'rence's en lecture
sont transfe're's avant l'exe'cution de cette instruction, pas
ne'cessairement juste avant, mais le plus souvent regroupe's avec ceux de
structures plus englobantes, de manie`re a` favoriser des possibilite's
d'effectuer des  communications groupe'es.
\item Les  e'le'ments  scalaires re'fe'rence's en e'criture sont
transfe're's juste apre`s l'exe'cution, tandis que les e'le'ments de
tableaux modifie's sont regroupe's avec ceux d'une structure plus
englobante pour favoriser les possibilite's de transferts d'e'le'ments
contigus, moins cou^teux. 
\end{itemize}

\subsubsection{Se'quence d'affectations}

\begin{itemize}
\item l'ensemble des e'le'ments re'fe'rence's en lecture est transfe're'
avant l'exe'cution de cette se'quence, si aucun des termes des indices
des fonctions d'acce`s aux e'le'ments d'un tableau ne de'pend d'une
variable scalaire qui est modifie'e dans cette se'quence line'aire (voir
Figure \ref{prog1}).  Sinon, une communication en re'ception est
ge'ne're'e juste avant l'instruction re'fe'renc,ant le tableau.

\begin{center}

\begin{figure}[hpt]
\begin{verbatim}
                                 A = ...
                                 C = T(A)
\end{verbatim}
\caption{Se'quence d'affectations - r\'ef\'erence indirecte}
\label{prog1}
\end{figure}
\end{center}


\item  Les  e'le'ments  scalaires re'fe'rence's en e'criture sont
transfe're's juste apre`s l'exe'cution, tandis que les e'le'ments de
tableaux modifie's sont regroupe's au niveau de la se'quence
d'instructions.
\end{itemize}

\subsubsection{Se'quence d'affectations- re'fe'rences indirectes}
Les scalaires e'tant distribue's sur tous les bancs me'moire, tous les 
processeurs e'mulant la me'moire partage'e disposent de la valeur de ces
scalaires.  Les e'le'ments de tableaux re'fe'rence's par des fonctions
d'acce`s nume'riques ou parame'triques, sous la condition que ce
parame`tre soit connue au de'but de l'exe'cution de la boucle, sont
traite's identiquement aux autres e'le'ments de tableaux dont la
fonction d'acce`s est line'aire. Ils repre'sentent un cas particulier
pour lequel la valeur des indices est nulle.
Par contre, les e'le'ments de tableaux dont la fonction d'acce`s est 
parame'trique et pour lesquelles la valeur du parame`tre n'est connue que
lors de l'exe'cution, doivent e^tre traite's diffe'remment. On distingue
aussi, pour des raisons d'optimisation, le cas ou` le nid de boucle est
totalement se'quentiel des autres cas:


\paragraph{\bf Le nid de boucles est totalement se'quentiel}

Chaque processeur de calcul de'duit des e'quations de line'arisation,
de'rivant de l'allocation cyclique sur les bancs me'moire des e'le'ments
de tableaux, la position me'moire (nume'ro de banc, nume'ro de la ligne
sur le banc et l'offset sur la ligne) de l'e'le'ment a
transfe'rer. Cette position est ensuite communique'e au banc me'moire
correspondant au nume'ro du processeur de calcul. Apre`s re'ception, le
banc me'moire qui posse`de la me'moire (simple test sur les valeurs
recues) envoie a` chacun des processeurs la valeur de l'e'le'ment.

\paragraph{\bf Le nid de boucles est partiellement ou totalement paralle`le}

Chaque processeur effectue des calculs qui peuvent e^tre diffe'rents de
ceux exe'cute's sur les autres processeurs de calcul. La valeur du
parame`tre peut donc e^tre diffe'rente pour chacun des
processeurs. Chaque processeur doit diffuser la position de l'e'le'ment
dont il a besoin a` l'ensemble des bancs me'moire. Le banc me'moire
correspondant renvoie en retour l'e'le'ment au processeur exe'cutant le
bloc correspondant.

\begin{verbatim} 
C   PROGRAMME d'ENTREE
C   programme comportant 
C      - des iterateurs i et j dans les calculs 
C      - des acces aux elements de tableaux parametres
C
C
      do 100 i = 1, size
         do 200 j = 1, size
            b(i,j) = (i-1)*size+(j-1)
            p = 1
            m = b(p,j)
            c(i,j) = b(i,j)/(size*size)
 200     continue
 100  continue
\end{verbatim}

\begin{verbatim} 
C   PROGRAME GENERE PAR LE PROTOTYPE
C   code distribue utilisant les iterateurs locaux L_I et L_J
C
         DO 99978 L_I = 0, 0
          DO 99979 L_J = 0, 1
          X3 = 1+I_0+L_I
          X4 = 1+2*J_0+L_J
          L_B_0_0(L_I,L_J) = (X3-1)*SIZE+X4-1
          P = 1
          L = (-1+20*J_0+10*L_J+P)/20
          X1 = (-1+20*J_0+10*L_J+P-20*L)/10
          O = -1+20*J_0+10*L_J+P-20*L-10*X1
          DOALL X2 = 0, 1
             CALL WP65_SEND_4(X2, X1, 1)
             CALL WP65_SEND_4(X2, L, 1)
             CALL WP65_SEND_4(X2, O, 1)
          ENDDO
          CALL WP65_RECEIVE_4(X1, L_B_0_0(P,L_J), 1)
          M = L_B_0_0(P,L_J)  
          L_C_0_0(L_I,L_J) = L_B_0_0(L_I,L_J)/(SIZE*SIZE)
99979 CONTINUE
99978 CONTINUE
\end{verbatim}

 Il faut noter que les variables qui sont {\em prive'es}\footnote{Une
variable est prive'e a` un nid de boucles, si sa valeur en entre'e de
boucle et sa valeur en sortie ne sont pas utilise'es dans le nid de
boucles en dehors de cette boucle; c'est le cas des variables
temporaires.}  a` une se'quence d'instructions n'impose une
communication qu'apre`s exe'cution de la se'quence d'instructions et pas
avant.


\subsubsection{Les nids de boucles totalement paralle`les}
 Les e'le'ments de tableaux ou scalaires re'fe'rence's en lecture
(respectivement en e'criture) sont transfe're's globalement
avant (respectivement apre`s) l'exe'cution. Il n'y a pas de conflits
me'moire entre les acce`s aux donne'es re'fe'rence'es puisque les
exe'cutions sont paralle`les.


\subsubsection{Les nids de boucles  se'quentielles} 

Les calculs se'quentiels sont exe'cute's sur un seul processeur. Tous
les e'le'ments re'fe'rence's en lecture (respectivement en e'criture)
peuvent e^tre transfe're'es globalement avant (respectivement apre`s)
l'exe'cution du nid de boucles, car les de'pendances sont respecte'es
par la se'quentialite' de l'exe'cution.


\paragraph{\bf Taille des blocs d'ite'rations se'quentielles}
D'une part pour respecter le mode`le SPMD et d'autre part pour limiter
les synchronisations et communications e'ventuelles internes a` un nid
de boucles, l'ensemble des exe'cutions se'quentielles sont effectue'es
par tous les processeurs. Deux solutions permettent de distribuer les
ite'rations se'quentielles sur l'ensemble des processeurs de calcul:
soit 1) on conside`re des blocs de taille e'gale a` la section de
tableau correspondant aux ite'rations de l'espace initial, soit 2) on
conside`re des blocs de taille unitaire. Dans le premier cas, le volume
des donne'es a` communiquer et a` allouer est important et correspond a`
l'ensemble des e'le'ments du tableau auxquels la fonction d'acce`s fait
re'fe'rence. Dans le second cas, seuls les e'le'ments de tableaux
re'fe'rence's par une valeur de l'indice sont communique's et alloue's.

Actuellement, le prototype  dimensionne les "blocs
se'quentiels" selon 2) pour les boucles
se'quentielles externes (e'ventuelles) aux boucles paralle`les et selon
1) pour les boucles se'quentielles internes
(e'ventuelles) aux boucles paralle`les.  Dans le cas de nid de
boucles totalement se'quentielles, un compromis mixte devra e^tre
introduit pour conserver des possibilite's de communications
vectorielles ou regroupe'es et limiter les transferts toujours cou^teux
de donne'es uniques.
    


\subsubsection{Les conditionnelles (pas encore inte'gre'es)}
Par de'faut, les deux branches de la
conditionnelle sont traite'es de manie`re inde'pendante selon les
structures pre'ce'demment de'crites auxquelles elles appartiennent. 
Toutefois, il est possible d'effectuer quelques optimisations pour les
deux cas suivants:
\begin{itemize}
\item La conditionnelle est une ine'quation line'aire de'pendante d'un
indice de boucle. Dans ce cas il est possible d'introduire cette
contrainte dans le domaine d'ite'rations et de ge'ne'rer un ou deux
nouveaux nids de boucles et leurs communications sans
conditionnelle. Les nids de boucles traduiront respectivement la branche
vraie et la branche fausse de la contrainte.

\item La conditionnelle est une fonction line'aire inde'pendante des
termes contenus  dans les deux branches du test. Cette contrainte est
alors extraite du nid de boucles et place'e a` l'exte'rieur. Les
communications sont ge'ne're'es pour chacune des branches
inde'pendamment. Toutefois, le  test est effectue' en dehors du nid de
boucles et pas une fois pour chacune des valeurs du domaine
d'ite'rations. 
\end{itemize}

\subsection{Code ge'ne're' pour la transposition d'une matrice M}
{\scriptsize
\begin{verbatim}
      SUBROUTINE WP65(PROC_ID)
      INTEGER*4 idiv
      EXTERNAL idiv
      INTEGER PROC_ID,BANK_ID,L,O,I_0,L_I,J_0,L_J,L_I_1,L_I_2,L_J_1,
     &L_J_2
      REAL*4 L_M_0_0(0:24,0:24),L_M_1_0(0:24,0:24),T

C     WP65 DISTRIBUTED CODE FOR TRANSP

C     To scan the tile set for WP65
      DO 99973 I_0 = PROC_ID, 3, 4
         DO 99974 J_0 = I_0, 3
            DOALL BANK_ID = 0, 3
               DO 99995 L_J = MAX(0, 1+25*I_0-25*J_0), 24
                  DO 99996 L = MAX(idiv(I_0+100*J_0, 16), idiv(4+101*
     &            I_0, 16)), idiv(96+I_0+100*J_0, 16)
                     L_I_1 = MAX(0, 100*BANK_ID-25*I_0-2500*J_0+400*L
     &               -100*L_J)
                     L_I_2 = MIN(24, 23-25*I_0+25*J_0, idiv(-1+100*
     &               BANK_ID-2525*I_0+400*L, 101), 99+100*BANK_ID-25*
     &               I_0-2500*J_0+400*L-100*L_J)
                     IF (L_I_1.LE.L_I_2) THEN
                        CALL WP65_RECEIVE_4(BANK_ID, L_M_0_0(L_I_1,
     &                  L_J), L_I_2-L_I_1+1)
                     ENDIF
99996                CONTINUE
99995             CONTINUE
99994          CONTINUE
            ENDDO
            DOALL BANK_ID = 0, 3
               DO 99990 L_I = 0, MIN(24, 23-25*I_0+25*J_0)
                  DO 99991 L = MAX(idiv(100*I_0+J_0, 16), idiv(301-
     &            100*BANK_ID+2525*I_0+101*L_I, 400)), MIN(idiv(96+
     &            100*I_0+J_0, 16), idiv(92+101*J_0, 16), idiv(-1*
     &            BANK_ID+25*I_0+L_I, 4))
                     L_J_1 = MAX(0, 1+25*I_0-25*J_0, 1+25*I_0-25*J_0+
     &               L_I, -9800+100*BANK_ID-25*J_0+400*L, 100*BANK_ID
     &               -2500*I_0-25*J_0+400*L-100*L_I)
                     L_J_2 = MIN(24, 99+100*BANK_ID-2500*I_0-25*J_0+
     &               400*L-100*L_I, 99+100*BANK_ID-25*J_0+400*L)
                     IF (L_J_1.LE.L_J_2) THEN
                        CALL WP65_RECEIVE_4(BANK_ID, L_M_1_0(L_J_1,
     &                  L_I), L_J_2-L_J_1+1)
                     ENDIF
99991                CONTINUE
99990             CONTINUE
99989          CONTINUE
            ENDDO
C           To scan each iteration of the current tile
            DO 99975 L_I = 0, 24
               DO 99976 L_J = MAX(1+25*I_0-25*J_0+L_I, 0), 24
                  T = L_M_0_0(L_I,L_J)
                  L_M_0_0(L_I,L_J) = L_M_1_0(L_J,L_I)
                  L_M_1_0(L_J,L_I) = T
200               CONTINUE
99976             CONTINUE
99975          CONTINUE
            DOALL BANK_ID = 0, 3
               DO 99984 L_J = MAX(0, 1+25*I_0-25*J_0), 24
                  DO 99985 L = MAX(idiv(I_0+100*J_0, 16), idiv(4+101*
     &            I_0, 16), idiv(3-BANK_ID+25*J_0+L_J, 4)), MIN(idiv(
     &            96+I_0+100*J_0, 16), idiv(99-BANK_ID, 4), idiv(-1-
     &            100*BANK_ID+2525*J_0+101*L_J, 400))
                     L_I_1 = MAX(0, 100*BANK_ID-25*I_0-2500*J_0+400*L
     &               -100*L_J)
                     L_I_2 = MIN(24, 23-25*I_0+25*J_0, idiv(-1+100*
     &               BANK_ID-2525*I_0+400*L, 101))
                     IF (L_I_1.LE.L_I_2) THEN
                        CALL WP65_SEND_4(BANK_ID, L_M_0_0(L_I_1,L_J)
     &                  , L_I_2-L_I_1+1)
                     ENDIF
99985                CONTINUE
99984             CONTINUE
99983          CONTINUE
            ENDDO
            DOALL BANK_ID = 0, 3
               DO 99978 L_I = 0, MIN(24, 23-25*I_0+25*J_0)
                  DO 99979 L = MAX(idiv(100*I_0+J_0, 16), idiv(301-
     &            100*BANK_ID+2525*I_0+101*L_I, 400)), MIN(idiv(96+
     &            100*I_0+J_0, 16), idiv(92+101*J_0, 16), idiv(-1*
     &            BANK_ID+25*I_0+L_I, 4))
                     L_J_1 = MAX(0, 1+25*I_0-25*J_0, 1+25*I_0-25*J_0+
     &               L_I)
                     L_J_2 = MIN(24, 99+100*BANK_ID-25*J_0+400*L, 99+
     &               100*BANK_ID-2500*I_0-25*J_0+400*L-100*L_I)
                     IF (L_J_1.LE.L_J_2) THEN
                        CALL WP65_SEND_4(BANK_ID, L_M_1_0(L_J_1,L_I)
     &                  , L_J_2-L_J_1+1)
                     ENDIF
99979                CONTINUE
99978             CONTINUE
99977          CONTINUE
            ENDDO
99974       CONTINUE
99973    CONTINUE
      RETURN
      END
\end{verbatim}
}
\newpage
{\scriptsize
\begin{verbatim}
      SUBROUTINE BANK(BANK_ID)
      INTEGER*4 idiv
      EXTERNAL idiv
      INTEGER PROC_ID,BANK_ID,L,O,I_0,L_I,J_0,L_J,O_1,O_2
      REAL*4 ES_M(0:99,0:25)

C     BANK DISTRIBUTED CODE FOR TRANSP

C     To scan the tile set for BANK
      DO 99971 I_0 = 0, 3
         PROC_ID = MOD(I_0, 4)
         DO 99972 J_0 = I_0, 3
            DO 99997 L_J = MAX(0, BANK_ID-25*J_0, 1+25*I_0-25*J_0), 
     &      24
               DO 99998 L = MAX(idiv(3-BANK_ID+25*J_0, 4), idiv(4-
     &         BANK_ID+25*I_0, 4)), idiv(24-BANK_ID+25*J_0, 4)
                  O_1 = MAX(0, -100*BANK_ID+2500*J_0-400*L+100*L_J, 
     &            -100*BANK_ID+25*I_0+2500*J_0-400*L+100*L_J)
                  O_2 = MIN(99, 9998-100*BANK_ID-400*L, 9924-100*
     &            BANK_ID+25*I_0-400*L, -1-100*BANK_ID+2525*J_0-400*L
     &            +101*L_J, 24-100*BANK_ID+25*I_0+2500*J_0-400*L+100*
     &            L_J)
                  IF (O_1.LE.O_2) THEN
                     CALL BANK_SEND_4(PROC_ID, ES_M(O_1,L), O_2-O_1+1
     &               )
                  ENDIF
99998             CONTINUE
99997          CONTINUE
            DO 99992 L_I = MAX(0, idiv(3+4*BANK_ID-100*I_0-J_0, 4)), 
     &      MIN(24, 23-25*I_0+25*J_0)
               DO 99993 L = MAX(0, idiv(12-4*BANK_ID+100*I_0+J_0, 16)
     &         ), MIN(idiv(96-4*BANK_ID+100*I_0+J_0, 16), idiv(92-4*
     &         BANK_ID+101*J_0, 16))
                  O_1 = MAX(0, 1-100*BANK_ID-400*L, 1-100*BANK_ID+
     &            2525*I_0-400*L+101*L_I, -100*BANK_ID+25*J_0-400*L, 
     &            -100*BANK_ID+2500*I_0+25*J_0-400*L+100*L_I)
                  O_2 = MIN(99, 2399-100*BANK_ID+2500*J_0, 24-100*
     &            BANK_ID+2500*I_0+25*J_0-400*L+100*L_I)
                  IF (O_1.LE.O_2) THEN
                     CALL BANK_SEND_4(PROC_ID, ES_M(O_1,L), O_2-O_1+1
     &               )
                  ENDIF
99993             CONTINUE
99992          CONTINUE
            DO 99986 L_J = MAX(0, BANK_ID-25*J_0, 1+25*I_0-25*J_0), 
     &      24
               DO 99987 L = MAX(idiv(3-BANK_ID+25*J_0, 4), idiv(4-
     &         BANK_ID+25*I_0, 4)), idiv(24-BANK_ID+25*J_0, 4)
                  DO 99988 O = MAX(0, -100*BANK_ID+25*I_0+2500*J_0-
     &            400*L+100*L_J), MIN(99, 24-100*BANK_ID+25*I_0+2500*
     &            J_0-400*L+100*L_J, 98-100*BANK_ID+2500*J_0-400*L+
     &            100*L_J, -1-100*BANK_ID+2525*J_0-400*L+101*L_J)
                     IF (idiv(202+100*BANK_ID+400*L+O, 101).LE.idiv(
     &               100+100*BANK_ID-25*I_0+400*L+O, 100).AND.idiv(
     &               101+100*BANK_ID+400*L+O, 100).LE.idiv(100+100*
     &               BANK_ID-25*I_0+400*L+O, 100).AND.idiv(175+100*
     &               BANK_ID-25*I_0+400*L+O, 100).LE.idiv(100+100*
     &               BANK_ID-25*I_0+400*L+O, 100)) THEN
                        CALL BANK_RECEIVE_4(PROC_ID, ES_M(O,L), 1)
                     ENDIF
99988                CONTINUE
99987             CONTINUE
99986          CONTINUE
            DO 99980 L_I = MAX(0, idiv(3+4*BANK_ID-100*I_0-J_0, 4)), 
     &      MIN(24, 23-25*I_0+25*J_0)
               DO 99981 L = MAX(0, idiv(12-4*BANK_ID+100*I_0+J_0, 16)
     &         ), MIN(idiv(96-4*BANK_ID+100*I_0+J_0, 16), idiv(92-4*
     &         BANK_ID+101*J_0, 16))
                  DO 99982 O = MAX(0, -100*BANK_ID+2500*I_0+25*J_0-
     &            400*L+100*L_I, 1-100*BANK_ID+2525*I_0-400*L+101*L_I
     &            ), MIN(99, 2399-100*BANK_ID+2500*J_0, 24-100*
     &            BANK_ID+2500*I_0+25*J_0-400*L+100*L_I)
                     IF (idiv(175+100*BANK_ID-25*J_0+400*L+O, 100)
     &               .LE.idiv(100+100*BANK_ID+400*L+O, 101).AND.idiv(
     &               175+100*BANK_ID-25*J_0+400*L+O, 100).LE.idiv(100
     &               +100*BANK_ID-25*J_0+400*L+O, 100)) THEN
                        CALL BANK_RECEIVE_4(PROC_ID, ES_M(O,L), 1)
                     ENDIF
99982                CONTINUE
99981             CONTINUE
99980          CONTINUE
99972       CONTINUE
99971    CONTINUE
      RETURN
      END
\end{verbatim}
}

\begin{thebibliography}{99}

\bibitem{Ancourt90} C. Ancourt,
{\em G\'en\'eration automatique de codes de transfert pour multiprocesseurs
\`a m\'emoires locales},
Th\`ese de doctorat, Universit\'e Paris~6, 1990


\bibitem{Creu95} 
Batrice Creusillet,
{\em Analyse de Flot de donnees: REGIONS de tableaux IN et OUT},
Confrence RenPar'7, Mons, Belgique, 30 mai-2 Juin 1995.

\bibitem{Dart93}  
Darte Alain and Robert Yves,
{\em Mapping Uniform Loop Nests onto Distributed
		Memory Architectures},Report jan. 93, LIP, ENS-Lyon.
	

\bibitem{Feau93}
Feautrier Paul,
{\em Toward Automatic Partitioning of Arrays on Distributed Memory
		Computers}, ICS'93, pages 175--184,
	

\bibitem{Gerndt89} 
H.M. Gerndt,
{\em Automatic parallelization for distributed-memory multiprocessing systems},
PhD, Bonn University, 1989

\bibitem{GuPr93}
Gupta Manish and Banerjee Prithviraj,
{\em Paradigme: A Compiler for Automatic Data Distribution on
		  Multicomputers}, 
	ICS'93, jul,pages 87--96

\bibitem{IrTr88} F. Irigoin, R. Triolet,
{\em Supernode Partitioning},
ACM Symposium on Principles of Programming Languages, San-Diego, 1988

\bibitem{IrJoTr91} F. Irigoin, P. Jouvelot, R. Triolet,
{\em Semantical Interprocedural Parallelization: An Overview of the PIPS Project},
ACM International Conference on Supercomputing (ICS'91), Cologne, 1991

\bibitem{Zhou94}
Lei Zhou,
``Static Evaluation of Fortran Program Complexity ``,
{\it The`se de l'Universite' Pierre et Marie Curie },
1994.



\end{thebibliography}
\end{document}
