

Source files copied in : master/Source/
Generated source files : master/src
###################### master/src/MASTER_PAR1.f ######################
!!
!! file for MASTER_PAR1.f
!!
      SUBROUTINE MASTER_PAR1(J, N, A, I)
      INTEGER J, N, I
      INTEGER A(1:N)
C     !$omp do
      CALL MASTER_PAR1_DO10(J, 1, N, N, A)                              0005
      PRINT *, "parallel1", A, I                                        0006
      I = -1                                                            0007
C     !$omp master
      CALL MASTER_PAR1_MASTER1(N, A, I)                                 0009
C     !$omp end master
C     !$omp barrier   ! no implict barrier for "END MASTER" directive
      CALL MASTER_PAR1_BARRIER1                                         0012


      PRINT *, "parallel2", A, I                                        0015
      END
###################### master/src/MASTER_PAR1_BARRIER1.f ######################
!!
!! file for MASTER_PAR1_BARRIER1.f
!!
      SUBROUTINE MASTER_PAR1_BARRIER1
      END
###################### master/src/MASTER_PAR1_BARRIER1_MPI.f ######################
!!
!! file for MASTER_PAR1_BARRIER1_MPI.f
!!
      SUBROUTINE MASTER_PAR1_BARRIER1_MPI
      implicit none
      include "STEP.h"
      CALL STEP_BARRIER                                                 0090
      END
###################### master/src/MASTER_PAR1_BARRIER1_OMP.f ######################
!!
!! file for MASTER_PAR1_BARRIER1_OMP.f
!!
      SUBROUTINE MASTER_PAR1_BARRIER1_OMP
!$OMP barrier

      END
###################### master/src/MASTER_PAR1_DO10.f ######################
!!
!! file for MASTER_PAR1_DO10.f
!!
      SUBROUTINE MASTER_PAR1_DO10(J, J_L, J_U, N, A)
      INTEGER J, J_L, J_U, N
      INTEGER A(1:N)
      DO 10 J = J_L, J_U                                                0004
         A(J) = J                                                       0005
         PRINT *, "do2", J                                              0006
10       CONTINUE                                                       0007
      END
###################### master/src/MASTER_PAR1_DO10_MPI.f ######################
!!
!! file for MASTER_PAR1_DO10_MPI.f
!!
      SUBROUTINE MASTER_PAR1_DO10_MPI(J, J_L, J_U, N, A)
      implicit none
      include "STEP.h"
      INTEGER*4 MAX_NB_REQUEST
      PARAMETER (MAX_NB_REQUEST = 2*MAX_NB_LOOPSLICES)
      INTEGER STEP_J_UP, STEP_J_LOW, J_UP, J_LOW, IDX, J, J_L, J_U, N
      INTEGER STEP_SR_A(IDX_SLICE_LOW:IDX_SLICE_UP, 1:1, 0:
     &MAX_NB_LOOPSLICES), STEP_J_LOOPSLICES(IDX_SLICE_LOW:
     &IDX_SLICE_UP, 1:MAX_NB_LOOPSLICES), A(1:N), STEP_REQUESTS(1:
     &MAX_NB_REQUEST)

      CALL STEP_COMPUTELOOPSLICES(J_L, J_U, 1, STEP_SIZE,               0098
     &MAX_NB_LOOPSLICES, STEP_J_LOOPSLICES)                             0098

C     Put array boundaries into region arrays (SR: Send region)
C     First dimension: lower and upper bounds of each slice
C     Second dimension: for each dimension of the original array
C     Third dimention: store the boundaries of the local chunk. The first element stores initial boundaries, then one element for each process
      STEP_SR_A(IDX_SLICE_LOW,1,0) = 1                                  0105
      STEP_SR_A(IDX_SLICE_UP,1,0) = N                                   0106

C     Region computation
      DO 99999 IDX = 1, MAX_NB_LOOPSLICES                               0109
         J_LOW = STEP_J_LOOPSLICES(IDX_SLICE_LOW,IDX)                   0110
         J_UP = STEP_J_LOOPSLICES(IDX_SLICE_UP,IDX)                     0111
C  <A(PHI1)-write-EXACT-{J_LOW<=PHI1, 1<=PHI1, PHI1<=J_UP, PHI1<=N}>
         STEP_SR_A(IDX_SLICE_LOW,1,IDX) = MAX(J_LOW, 1)                 0113
         STEP_SR_A(IDX_SLICE_UP,1,IDX) = MIN(J_UP, N)                   0114
99999    CONTINUE                                                       0115

C     Where work is done...
      STEP_J_LOW = STEP_J_LOOPSLICES(IDX_SLICE_LOW,STEP_RANK+1)         0118
      STEP_J_UP = STEP_J_LOOPSLICES(IDX_SLICE_UP,STEP_RANK+1)           0119
      CALL MASTER_PAR1_DO10(J, STEP_J_LOW, STEP_J_UP, N, A)             0120

C     Communicating data to other nodes
C     3 communication shemes for all-to-all personalized broadcast :
C     STEP_NONBLOCKING, STEP_BLOCKING1 and STEP_BLOCKING2.
C     A nonblocking algo increment STEP_Nb_Request.
      STEP_NBREQUEST = 0                                                0126
      CALL STEP_ALLTOALLREGION_I4(1, STEP_SIZE, STEP_SR_A,              0127
     &STEP_SIZEREGION(1, STEP_SR_A(IDX_SLICE_LOW,1,0)), A, 0,           0127
     &MAX_NB_REQUEST, STEP_REQUESTS, STEP_NBREQUEST, STEP_NONBLOCKING   0127
     &)                                                                 0127
C     If STEP_Nb_Request equals 0, STEP_WAITALL does nothing
      CALL STEP_WAITALL(STEP_NBREQUEST, STEP_REQUESTS)                  0132
      END
###################### master/src/MASTER_PAR1_DO10_OMP.f ######################
!!
!! file for MASTER_PAR1_DO10_OMP.f
!!
      SUBROUTINE MASTER_PAR1_DO10_OMP(J, J_L, J_U, N, A)
      INTEGER J, J_L, J_U, N
      INTEGER A(1:N)
!$OMP do
      DO 10 J = J_L, J_U                                                0005
         A(J) = J                                                       0006
         PRINT *, "do2", J                                              0007
10       CONTINUE                                                       0008
!$OMP end do
      END
###################### master/src/MASTER_PAR1_MASTER1.f ######################
!!
!! file for MASTER_PAR1_MASTER1.f
!!
      SUBROUTINE MASTER_PAR1_MASTER1(N, A, I)
      INTEGER N, I
      INTEGER A(1:N)
      PRINT *, "master", A, I                                           0004

      DO 20 I = 1, N                                                    0006
         A(I) = 2*I                                                     0007
20       CONTINUE                                                       0008
      END
###################### master/src/MASTER_PAR1_MASTER1_MPI.f ######################
!!
!! file for MASTER_PAR1_MASTER1_MPI.f
!!
      SUBROUTINE MASTER_PAR1_MASTER1_MPI(N, A, I)
      implicit none
      include "STEP.h"
      INTEGER*4 MAX_NB_REQUEST
      PARAMETER (MAX_NB_REQUEST = 2*2*MAX_NB_LOOPSLICES)
      INTEGER N, I
      INTEGER STEP_SR_A(IDX_SLICE_LOW:IDX_SLICE_UP, 1:1), A(1:N), 
     &STEP_REQUESTS(1:MAX_NB_REQUEST)
      STEP_SR_A(IDX_SLICE_LOW,1) = 1                                    0095
      STEP_SR_A(IDX_SLICE_UP,1) = N                                     0096
      IF (STEP_RANK.EQ.0) CALL MASTER_PAR1_MASTER1(N, A, I)             0097

C     Communicating data to other nodes
C     3 communication shemes for all-to-all personalized broadcast :
C     STEP_NONBLOCKING, STEP_BLOCKING1 and STEP_BLOCKING2.
C     A nonblocking algo increment STEP_Nb_Request.
      STEP_NBREQUEST = 0                                                0103
      CALL STEP_MASTERTOALLSCALAR_I4(I, 0)                              0104
      CALL STEP_MASTERTOALLREGION_I4(A, 1, STEP_SR_A, STEP_SIZEREGION   0105
     &(1, STEP_SR_A(IDX_SLICE_LOW,1)), A, MAX_NB_REQUEST,               0105
     &STEP_REQUESTS, STEP_NBREQUEST, 0)                                 0105
C     If STEP_Nb_Request equals 0, STEP_WAITALL does nothing
      CALL STEP_WAITALL(STEP_NBREQUEST, STEP_REQUESTS)                  0109
      END
###################### master/src/MASTER_PAR1_MASTER1_OMP.f ######################
!!
!! file for MASTER_PAR1_MASTER1_OMP.f
!!
      SUBROUTINE MASTER_PAR1_MASTER1_OMP(N, A, I)
      INTEGER N, I
      INTEGER A(1:N)
!$OMP master
      PRINT *, "master", A, I                                           0005

      DO 20 I = 1, N                                                    0007
         A(I) = 2*I                                                     0008
20       CONTINUE                                                       0009
!$OMP end master
      END
###################### master/src/MASTER_PAR1_OMP.f ######################
!!
!! file for MASTER_PAR1_OMP.f
!!
      SUBROUTINE MASTER_PAR1_OMP(J, N, A, I)
      INTEGER J, N, I
      INTEGER A(1:N)
!$OMP parallel
C     !$omp do
      CALL MASTER_PAR1_DO10(J, 1, N, N, A)                              0006
      PRINT *, "parallel1", A, I                                        0007
      I = -1                                                            0008
C     !$omp master
      CALL MASTER_PAR1_MASTER1(N, A, I)                                 0010
C     !$omp end master
C     !$omp barrier   ! no implict barrier for "END MASTER" directive
      CALL MASTER_PAR1_BARRIER1                                         0013


      PRINT *, "parallel2", A, I                                        0016
!$OMP end parallel
      END
###################### master/src/STEP.f ######################


* Copyright 2007, 2008 Alain Muller, Frederique Silber-Chaussumier
*
*This file is part of STEP.
*
*The program is distributed under the terms of the GNU General Public
*License.




      subroutine STEP_get_myloopslice(i)
      INTEGER STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE
      COMMON /MYLOOPSLICE/ STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE

      i = STEP_MYLOOPSLICE
      end

      subroutine STEP_get_i_low(i)
      INTEGER STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE
      COMMON /MYLOOPSLICE/ STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE

      i = STEP_I_LOW
      END

      subroutine STEP_get_i_up(i)
      INTEGER STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE
      COMMON /MYLOOPSLICE/ STEP_I_LOW, STEP_I_UP, STEP_MYLOOPSLICE

      i = STEP_I_UP
      end

      subroutine STEP_Init()
      implicit none
      include 'STEP.h'
      integer*4 ierr,iBidon
      logical Initialized
      call MPI_Initialized(Initialized,ierr )
      if (.not. Initialized) then
         call MPI_Init ( ierr )
!     print *, STEP_size, STEP_rank
      end if
      call MPI_Comm_size ( MPI_COMM_WORLD, STEP_size, ierr )
      call MPI_Comm_rank ( MPI_COMM_WORLD, STEP_rank, ierr )      

      if (STEP_size .gt. STEP_MAX_NBNODE) then
         print *,"STEP_Init() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) STEP_size = ",STEP_size
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, ierr)
      end if
      end
      
      subroutine STEP_Finalize()
      implicit none
      include 'mpif.h'
      integer*4 ierr
      call MPI_Finalize(ierr)
      end

      subroutine STEP_Barrier()
      implicit none
      include 'mpif.h'
      integer*4 ierr
      call MPI_BARRIER(MPI_COMM_WORLD,ierr)
      end

      subroutine STEP_Get_size(size)
      implicit none
      include 'mpif.h'
      integer*4 size,ierr
      call MPI_Comm_size (  MPI_COMM_WORLD, size, ierr )
      end

      subroutine STEP_Get_rank(rank)
      implicit none
      include 'mpif.h'
      integer*4 rank,ierr
      call MPI_Comm_rank ( MPI_COMM_WORLD, rank, ierr )
      end
	
      function STEP_Get_thread_num()
      implicit none
      integer STEP_Get_thread_num
      call STEP_Get_rank(STEP_Get_thread_num)
      end

      subroutine STEP_ComputeLoopSlices(from,to,step,nb,size,bounds)
      implicit none
      integer F,T,from,to,step,nb,size
      parameter (F=1,T=2)
      integer bounds(F:T,size)
      integer nb_indices,nb_i,nb_e,i
      
c      print *,'from',from,'to',to,'step',step,'nb',nb,'size',size,
c     * 'bounds'

      do 5 i=1,size
         bounds(F,i)=-1
         bounds(T,i)=-1
 5    continue
      if (((step.GT.0) .AND. ((to-from).LT.step)) .OR.
     *     ((step.LT.0) .AND. ((from-to).LT.-step))) then ! pas d'iteration
c         print *,'pas d''iteration'
         do 10 i=1,nb
            bounds(F,i)=to
 10         bounds(T,i)=from
      else
         nb_indices=(to-from)/step+1
c         print *,nb_indices,' iterations'
         if (nb_indices.LE.nb) then ! une iteration par noeud
c            print *,'au plus une iteration par noeud'
            bounds(F,1)=from
            bounds(T,1)=from
            do 20 i=2,nb_indices
               bounds(F,i)= bounds(F,i-1)+step
 20            bounds(T,i)= bounds(F,i)
            do 25 i=nb_indices+1,nb ! plus d'iteration pour les autres noeuds
               bounds(F,i)= to
 25            bounds(T,i)= from
         else                   !au moins un noeud avec deux iterations
            nb_i=MOD(nb_indices,nb)
            nb_e=(nb_indices-nb_i)/nb
            if (nb_i.EQ.0) then
               nb_i=nb
               nb_e=nb_e-1
            endif
            bounds(F,1)=from
            bounds(T,1)=from+nb_e*step
            do 30 i=2,nb_i
c               print *, 'do30',i
               bounds(F,i)=bounds(T,i-1)+step
 30            bounds(T,i)=bounds(F,i)+nb_e*step
            do 35 i=nb_i+1,nb
c               print *,'do35',i
               bounds(F,i)=bounds(T,i-1)+step
 35            bounds(T,i)=bounds(F,i)+(nb_e-1)*step
         endif
      endif
    
c      print *,bounds
      end
      
******************************************
c
c     procédure produisant type_sub_region (handler de  type MPI) 
c     représentant la sous-region subregion définit dans l'espace d'indice region
c     d'élément de type type_region (handler de type MPI)
c
      subroutine type_subRegion(dim,region,sub_region,type_region,
     &     type_sub_region)
      implicit none
      include 'STEP.h'
      integer dim               !< nombre de dimension de l'espace
      integer L,U
      parameter (L=1, U=2)
      integer region(L:U,dim)      !< borne min/max des indices selon chaque dimension
      integer*4 sub_region(L:U,dim)      !< borne min/max des indices selon chaque dimension
      integer type_region       !< type MPI décrivant un element de la region
      integer type_sub_region   !> type MPI décrivant la sous-region
      
      integer*4 array_type(0:STEP_MAX_DIM),data_size(0:STEP_MAX_DIM),i
      integer*4 iBidon,IERROR
      logical valide

      if (dim .gt. STEP_MAX_DIM) then
         print *,"type_subRegion() : STEP_MAX_DIM 
     &trop petit (update STEP.h) dim = ",dim
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, IERROR)
      end if
!      print *,'dim=',dim,'region =',region,
!     &     'sub_region =',sub_region

      array_type(0)=type_region
      call MPI_TYPE_EXTENT(array_type(0),data_size(0),IERROR)
      i=1
      valide=.TRUE.
      do while (valide .AND. (i.LE.dim))
c     verification des définitions des régions/sous-regions
         if (.NOT.((region(L,i) .LE. sub_region(L,i)) .AND.
     &        (sub_region(L,i) .LE. sub_region(U,i)) .AND.
     &        (sub_region(U,i) .LE. region(U,i)))) then
            valide=.FALSE.
            array_type(dim)=MPI_DATATYPE_NULL
         else
            data_size(i)=data_size(i-1)*
     &           (region(U,i)-region(L,i)+1)
            call MPI_TYPE_HVECTOR(
     &           sub_region(U,i)-sub_region(L,i)+1, 1,
     &           data_size(i-1), array_type(i-1), array_type(i),IERROR)
         endif
         i=i+1
      enddo
      type_sub_region=array_type(dim)
      end
      
******************************************
c     
c     retourne la taille d'un tableau donc l'espace des indices 
c     est définit par region
      
      function STEP_SizeRegion(dim,region)
      implicit none
      integer dim,d,STEP_sizeRegion
      integer L,U
      parameter (L=1, U=2)
      integer region(L:U,dim)
      STEP_sizeRegion=1
      do 10 d=1,dim
         STEP_sizeRegion=STEP_sizeRegion*(region(U,d)-region(L,d)+1)
 10   continue
      end

c     
c     retourne l'indice dans un tableau linéaire dont l'indice commence à 1,
c     d'une case de cordonnées coords définit pour l'espace d'indice  region 
c     
      function indice(dim,coords,region)
      implicit none
      integer dim,d,indice
      integer coords(dim)
      integer L,U
      parameter (L=1, U=2)
      integer region(L:U,dim)
      indice = 0
      do 10 d=dim,1,-1
         indice=indice*(region(U,d)-region(L,d)+1)+
     &        coords(d)-region(L,d)
 10   continue
      indice=indice+1
      end
      
c     
c     retourne l'indice dans un tableau linéaire dont l'indice commence à 1,
c     de l'origine d'une sous-region subregion, définit dans l'espace d'indice
c     definit par region
c     
      function origine(dim,region,subregion)
      implicit none
      include 'STEP.h'
      integer dim,d,origine,indice
      integer L,U
      parameter (L=1,U=2)
      integer region(L:U,dim)
      integer subregion(L:U,dim)
      integer coords(STEP_MAX_DIM)
      integer*4 iBidon, IERROR

      if (dim .gt. STEP_MAX_DIM) then
         print *,"origine() : STEP_MAX_DIM 
     &trop petit (update STEP.h) dim = ",dim
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, IERROR)
      end if
      do 10 d=1,dim
         coords(d)=subregion(L,d)
 10   continue
      origine=indice(dim,coords,region)
!      print *,'\nregion=',region,'\nsubregion=',subregion,
!     &     '\ncoords=',coords,'\norigine=',origine
      end

******************************************
       subroutine STEP_WaitAll(NbReq,Request)
      implicit none
      include 'STEP.h'
      integer*4 NbReq,Request(NbReq)
      integer*4 Status(MPI_STATUS_SIZE,STEP_MAX_NBREQ)
      integer*4 iErr,iBidon

      if (NbReq .gt. STEP_MAX_NBREQ) then
         print *,"STEP_WaitAll() : STEP_MAX_NBREQ
     &trop petit (update STEP.h) NbReq = ",NbReq
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      call MPI_BARRIER(MPI_COMM_WORLD,iErr)
      if (iErr .ne. MPI_SUCCESS) then
         print *,'STEP_WaitAll() : MPI_BARRIER() Pb'
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if (NbReq .ne. 0) then
         call mpi_waitall(NbReq,Request,Status,iErr)
         if (iErr .ne. MPI_SUCCESS) then
            print *,'STEP_WaitAll() : mpi_wait_all() Pb'
            call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
         end if
      end if
      end

      subroutine AlltoAll_nBlocking_I1(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_I1(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      integer*1 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_I1(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      integer*1 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_INTEGER1,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_I1(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer*1 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_INTEGER1,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_I1(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      integer*1 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_INTEGER1,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_I1(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*1 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_I1() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER1,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_I1'
         call AlltoAll_nBlocking_I1(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_I1'
            call AlltoAll_even_I1(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_I1'
            call AlltoAll_odd_I1(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_I1'
            call AlltoAll_even2_I1(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_I1'
            call AlltoAll_odd2_I1(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_I1(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_I1(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*1 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_I1(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_I1(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end
      subroutine AlltoAll_nBlocking_I2(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_I2(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      integer*2 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_I2(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      integer*2 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_INTEGER2,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_I2(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer*2 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_INTEGER2,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_I2(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      integer*2 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_INTEGER2,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_I2(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*2 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_I2() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER2,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_I2'
         call AlltoAll_nBlocking_I2(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_I2'
            call AlltoAll_even_I2(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_I2'
            call AlltoAll_odd_I2(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_I2'
            call AlltoAll_even2_I2(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_I2'
            call AlltoAll_odd2_I2(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_I2(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_I2(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*2 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_I2(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_I2(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end
      subroutine AlltoAll_nBlocking_I4(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_I4(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      integer*4 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_I4(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      integer*4 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_INTEGER4,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_I4(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer*4 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_INTEGER4,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_I4(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      integer*4 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_INTEGER4,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_I4(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*4 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_I4() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER4,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_I4'
         call AlltoAll_nBlocking_I4(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_I4'
            call AlltoAll_even_I4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_I4'
            call AlltoAll_odd_I4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_I4'
            call AlltoAll_even2_I4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_I4'
            call AlltoAll_odd2_I4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_I4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_I4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*4 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_I4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_I4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end    
      subroutine AlltoAll_nBlocking_I8(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_I8(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      integer*8 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_I8(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      integer*8 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_INTEGER8,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_I8(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer*8 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_INTEGER8,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_I8(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      integer*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_INTEGER8,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_I8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_I8() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_I8'
         call AlltoAll_nBlocking_I8(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_I8'
            call AlltoAll_even_I8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_I8'
            call AlltoAll_odd_I8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_I8'
            call AlltoAll_even2_I8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_I8'
            call AlltoAll_odd2_I8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_I8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_I8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*8 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_I8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_I8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end
      subroutine AlltoAll_nBlocking_R4(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_R4(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      real*4 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_R4(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      real*4 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_REAL4,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_R4(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      real*4 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_REAL4,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_R4(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      real*4 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_REAL4,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_R4(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      real*4 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_R4() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_REAL4,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_R4'
         call AlltoAll_nBlocking_R4(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_R4'
            call AlltoAll_even_R4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_R4'
            call AlltoAll_odd_R4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_R4'
            call AlltoAll_even2_R4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_R4'
            call AlltoAll_odd2_R4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_R4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_R4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      real*4 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_R4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_R4(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end          
      subroutine AlltoAll_nBlocking_R8(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_R8(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      real*8 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_R8(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      real*8 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_REAL8,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_R8(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      real*8 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_REAL8,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_R8(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      real*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_REAL8,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_R8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      real*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_R8() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_REAL8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_R8'
         call AlltoAll_nBlocking_R8(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_R8'
            call AlltoAll_even_R8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_R8'
            call AlltoAll_odd_R8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_R8'
            call AlltoAll_even2_R8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_R8'
            call AlltoAll_odd2_R8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_R8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_R8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      real*8 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_R8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_R8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end          
      subroutine AlltoAll_nBlocking_C8(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_C8(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      complex*8 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_C8(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      complex*8 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_COMPLEX8,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_C8(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      complex*8 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_COMPLEX8,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_C8(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      complex*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_COMPLEX8,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_C8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      complex*8 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_C8() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_COMPLEX8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_C8'
         call AlltoAll_nBlocking_C8(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_C8'
            call AlltoAll_even_C8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_C8'
            call AlltoAll_odd_C8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_C8'
            call AlltoAll_even2_C8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_C8'
            call AlltoAll_odd2_C8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_C8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_C8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      complex*8 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_C8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_C8(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end    
      subroutine AlltoAll_nBlocking_C16(nb_node,types,origines,
     &     size,array,tag,max_nb_request,requests,nb_request)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 node,iErr,STEP_Rank
      integer max_nb_request
      integer*4 requests(max_nb_request)
      integer nb_request
      integer*4 tag
      CALL STEP_Get_rank(STEP_Rank)
      do node=0,nb_node-1
         if ((node.NE.STEP_Rank).AND.
     &        (types(node).NE.MPI_DATATYPE_NULL)) then
            nb_request=nb_request+1
            call MPI_IRECV(array(origines(node)),1,types(node),node,
     &           tag, MPI_COMM_WORLD,requests(nb_request),iErr)
         endif
      enddo
      if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
         do node=0,nb_node-1
            if (node.NE.STEP_Rank) then
               nb_request=nb_request+1
               call MPI_ISEND(array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,
     &              tag, MPI_COMM_WORLD,requests(nb_request),iErr)
            endif
         enddo
      endif
      end
      subroutine STEP_InitReduction_C16(variable,
     &     variable_reduc,operator)
      implicit none
      include 'STEP.h'
      complex*16 variable,variable_reduc
      integer operator
      variable_reduc=variable
      select case(operator)
      case(STEP_SUM)
         variable=0
      case(STEP_PROD)
         variable=1
      end select
      end
      subroutine STEP_Reduction_C16(variable, variable_reduc,
     &     operator)
      implicit none
      include 'STEP.h'
      complex*16 variable,variable_reduc,tmp_result
      integer*4 operator,iErr
      call MPI_ALLREDUCE (variable,tmp_result,1,MPI_COMPLEX16,
     &     operator,MPI_COMM_WORLD,iErr)
      select case(operator)
      case(STEP_SUM)
         variable=variable_reduc+tmp_result
      case(STEP_PROD)
         variable=variable_reduc*tmp_result
      end select
      end
      subroutine STEP_MasterToAllScalar_C16(scalar,
     &     algorithm)
      implicit none
      include 'mpif.h'
      complex*16 scalar
      integer*4 iErr
      integer algorithm
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(scalar,1,MPI_COMPLEX16,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_MasterToAllRegion_C16(array,
     &     dim,regions,size,
     &     algorithm)
      implicit none
      include 'mpif.h'
      integer dim,size
      complex*16 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer regions(L:U,1:dim,0:1) !description de l'espace d'indice des differentes regions
                                 !regions(:,:) decrit l'espace d'indice du tableau array non linearise
      integer*4 iErr
      integer algorithm
      integer STEP_Rank, STEP_Size
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      integer STEP_SizeRegion   !resultat fonction de calcul
      integer*4 type
      call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,B_),
     &     MPI_COMPLEX16,type)
      if(type.NE.MPI_DATATYPE_NULL) then
         call MPI_TYPE_COMMIT(type,iErr)
      endif
      select case (algorithm)
      case DEFAULT
         call MPI_BCAST(array(origine(dim,regions(L,1,B_),
     &        regions(L,1,B_))),1,type,0,MPI_COMM_WORLD,iErr)
      end select
      end
      subroutine STEP_AlltoAllRegion_C16(dim,
     &     nb_regions,regions,
     &     size,array,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      complex*16 array(1:size)     !tableau array linearise
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**//**/_C16() : STEP_MAX_NBNODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_COMPLEX16,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (NBLOCKING)
c      print *,STEP_Rank,') AlltoAll_nBlocking_C16'
         call AlltoAll_nBlocking_C16(STEP_Size,types,origines,
     &        STEP_SizeRegion(dim,regions(L,1,B_)),array,tag,
     &        max_nb_request,requests,nb_request)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_C16'
            call AlltoAll_even_C16(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd_C16'
            call AlltoAll_odd_C16(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_C16'
            call AlltoAll_even2_C16(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         else
c      print *,STEP_Rank,') AlltoAll_odd2_C16'
            call AlltoAll_odd2_C16(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           )
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_C16(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(array(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_C16(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      complex*16 array(size)
      integer*4 tag
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(array(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(array(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
         end select
      enddo
      end
      subroutine AlltoAll_odd_C16(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(array(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_C16(nb_node,types,origines,
     &     size,array,tag )
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              array(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
            end select
         endif
      enddo
      end
      subroutine STEP_InitInterlaced_I1(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      integer*1 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_I1(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      integer*1 initial(1:size)   !tableau initial linéarisé
      integer*1 in(1:size)        !tableau in linéarisé
      integer*1 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_I1(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*1 array(1:size)     !tableau array linearise
      integer*1 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_I1() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER1,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_I1'
            call AlltoAll_even_Merge_I1(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_I1'
            call AlltoAll_odd_Merge_I1(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_I1'
            call AlltoAll_even2_Merge_I1(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_I1'
            call AlltoAll_odd2_Merge_I1(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_I1(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*1 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I1(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I1(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_I1(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*1 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*1 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I1(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I1(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I1(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_I1(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*1 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I1(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I1(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_I1(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*1 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*1 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I1(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I1(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end
      subroutine STEP_InitInterlaced_I2(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      integer*2 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_I2(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      integer*2 initial(1:size)   !tableau initial linéarisé
      integer*2 in(1:size)        !tableau in linéarisé
      integer*2 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_I2(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*2 array(1:size)     !tableau array linearise
      integer*2 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_I2() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER2,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_I2'
            call AlltoAll_even_Merge_I2(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_I2'
            call AlltoAll_odd_Merge_I2(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_I2'
            call AlltoAll_even2_Merge_I2(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_I2'
            call AlltoAll_odd2_Merge_I2(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_I2(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*2 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I2(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I2(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_I2(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*2 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*2 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I2(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I2(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I2(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_I2(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*2 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I2(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I2(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_I2(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*2 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*2 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I2(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I2(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end
      subroutine STEP_InitInterlaced_I4(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      integer*4 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_I4(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      integer*4 initial(1:size)   !tableau initial linéarisé
      integer*4 in(1:size)        !tableau in linéarisé
      integer*4 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_I4(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*4 array(1:size)     !tableau array linearise
      integer*4 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_I4() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER4,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_I4'
            call AlltoAll_even_Merge_I4(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_I4'
            call AlltoAll_odd_Merge_I4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_I4'
            call AlltoAll_even2_Merge_I4(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_I4'
            call AlltoAll_odd2_Merge_I4(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_I4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*4 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I4(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I4(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_I4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*4 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I4(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I4(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I4(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_I4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*4 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I4(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I4(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_I4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*4 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I4(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I4(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end    
      subroutine STEP_InitInterlaced_I8(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      integer*8 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_I8(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      integer*8 initial(1:size)   !tableau initial linéarisé
      integer*8 in(1:size)        !tableau in linéarisé
      integer*8 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_I8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      integer*8 array(1:size)     !tableau array linearise
      integer*8 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_I8() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_INTEGER8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_I8'
            call AlltoAll_even_Merge_I8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_I8'
            call AlltoAll_odd_Merge_I8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_I8'
            call AlltoAll_even2_Merge_I8(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_I8'
            call AlltoAll_odd2_Merge_I8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_I8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*8 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I8(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I8(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_I8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      integer*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*8 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_I8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_I8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_I8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_I8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_I8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      integer*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_I8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end
      subroutine STEP_InitInterlaced_R4(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      real*4 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_R4(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      real*4 initial(1:size)   !tableau initial linéarisé
      real*4 in(1:size)        !tableau in linéarisé
      real*4 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_R4(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      real*4 array(1:size)     !tableau array linearise
      real*4 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_R4() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_REAL4,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_R4'
            call AlltoAll_even_Merge_R4(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_R4'
            call AlltoAll_odd_Merge_R4(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_R4'
            call AlltoAll_even2_Merge_R4(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_R4'
            call AlltoAll_odd2_Merge_R4(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_R4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*4 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R4(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R4(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_R4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      real*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*4 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_R4(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_R4(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R4(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_R4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*4 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_R4(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_R4(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_R4(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*4 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*4 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R4(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R4(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end          
      subroutine STEP_InitInterlaced_R8(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      real*8 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_R8(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      real*8 initial(1:size)   !tableau initial linéarisé
      real*8 in(1:size)        !tableau in linéarisé
      real*8 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_R8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      real*8 array(1:size)     !tableau array linearise
      real*8 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_R8() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_REAL8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_R8'
            call AlltoAll_even_Merge_R8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_R8'
            call AlltoAll_odd_Merge_R8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_R8'
            call AlltoAll_even2_Merge_R8(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_R8'
            call AlltoAll_odd2_Merge_R8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_R8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*8 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R8(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R8(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_R8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      real*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*8 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_R8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_R8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_R8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_R8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_R8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_R8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_R8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      real*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      real*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_R8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end          
      subroutine STEP_InitInterlaced_C8(size,array,array_initial,array_b
     >uffer)
      implicit none
      integer size,i
      complex*8 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_C8(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      complex*8 initial(1:size)   !tableau initial linéarisé
      complex*8 in(1:size)        !tableau in linéarisé
      complex*8 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_C8(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      complex*8 array(1:size)     !tableau array linearise
      complex*8 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_C8() : STEP_MAX_NBNO
     >DE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_COMPLEX8,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_C8'
            call AlltoAll_even_Merge_C8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_C8'
            call AlltoAll_odd_Merge_C8(STEP_Size,types,origines,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_C8'
            call AlltoAll_even2_Merge_C8(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_C8'
            call AlltoAll_odd2_Merge_C8(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_C8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*8 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C8(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C8(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_C8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      complex*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*8 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_C8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_C8(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C8(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_C8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_C8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_C8(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_C8(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*8 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*8 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C8(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end    
      subroutine STEP_InitInterlaced_C16(size,array,array_initial,array_
     >buffer)
      implicit none
      integer size,i
      complex*16 array(size),array_initial(size),array_buffer(size)
      do i=1,size
         array_initial(i)=array(i)
         array_buffer(i)=array(i)
      end do
      end
      subroutine MergeRegion_C16(dim,
     &     nb_regions,regions,id_sub_region,
     &     size,initial,in,out)
      implicit none
      include 'mpif.h'
      integer dim               !nombre de dimension du tableau array avant linéarisation
      integer size              !taille du tableau array
      complex*16 initial(1:size)   !tableau initial linéarisé
      complex*16 in(1:size)        !tableau in linéarisé
      complex*16 out(1:size)       !tableau out linéarisé
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      integer id_sub_region     !identifiant de la région devant être envoyé
      integer profil(L:U,1:7)   !profile de la sous-region formate à 7 dimensions
      integer d,d1,d2,d3,d4,d5,d6,d7 !indices de parcours des dimensions
      integer coords(1:7),indice,i
      integer*4 ierr
      !mise a jour du profile
      do 10 d=1,dim
         profil(L,d)=regions(L,d,id_sub_region)
         profil(U,d)=regions(U,d,id_sub_region)
 10   continue
      do 20 d=dim+1,7
         profil(L,d)=0
         profil(U,d)=0
 20   continue
c      print *,'regions=',regions
c      print *,'initial=',initial
c      print *,'in=',in
c      print *,'out=',out
c      print *,'profil=',profil
      !parcours de la sous_region
      do 30 d7=profil(L,7),profil(U,7)
         coords(7)=d7
      do 30 d6=profil(L,6),profil(U,6)
         coords(6)=d6
      do 30 d5=profil(L,5),profil(U,5)
         coords(5)=d5
      do 30 d4=profil(L,4),profil(U,4)
         coords(4)=d4
      do 30 d3=profil(L,3),profil(U,3)
         coords(3)=d3
      do 30 d2=profil(L,2),profil(U,2)
         coords(2)=d2
      do 30 d1=profil(L,1),profil(U,1)
         coords(1)=d1
         !comparaison case à case pour la sous-region
         i=indice(dim,coords,regions(L,1,B_))
!         print *,'i',i,' coord :',coords,' comp : ',initial(i),
!     &        in(i),out(i)
         if((in(i).NE.initial(i)).AND.(in(i).NE.out(i)) ) then
            if(out(i).NE.initial(i)) then
               print *,'\n** Concurrent Access **\n'
               call MPI_ABORT(MPI_COMM_WORLD,2,ierr)
            else
               out(i)=in(i)
            endif
         endif
 30   continue
      end
      subroutine STEP_AlltoAllRegion_Merge_C16(dim,
     &     nb_regions,regions,
     &     size,array,
     &     initial,buffer,
     &     tag,
     &     max_nb_request,requests,nb_request,
     &     algorithm)
      implicit none
      include 'STEP.h'
      integer NBLOCKING,BLOCKING1,BLOCKING2
      parameter (NBLOCKING=0,BLOCKING1=1,BLOCKING2=2)
      integer dim               !nombre de dimension du tableau array avant linearisation
      integer size              !taille du tableau array
      complex*16 array(1:size)     !tableau array linearise
      complex*16 initial(1:size),buffer(1:size)
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'element du tableau region +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des differentes regions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linearise
      integer tag               !tag MPI pour le message
      integer max_nb_request
      integer requests(max_nb_request)
      integer nb_request
      integer algorithm
      integer*4 types(STEP_MAX_NBNODE)
      integer origines(STEP_MAX_NBNODE)
      integer id_region,idProc
      integer*4 iErr,iBidon
      integer origine           !resultat fonction de calcul
      integer type_sub_region   !resultat fonction de calcul
      if (nb_regions .gt. STEP_MAX_NBNODE) then
         print *,"STEP_AlltoAllRegion/**/_Merge/**/_C16() : STEP_MAX_NBN
     >ODE
     &trop petit (update STEP.h) nb_regions = ",nb_regions
         call MPI_ABORT(MPI_COMM_WORLD, iBidon, iErr)
      end if
      if(nb_request.GE.max_nb_request-2*(STEP_Size-1)) then
         print *,"STEP error : requests array too small"
         stop
      endif
      ! initialisation des types MPI et du tableau des origines des regions
      do id_region=1,nb_regions
         origines(id_region)=origine(dim,regions(L,1,B_),
     &        regions(L,1,id_region))
c         print *, STEP_Rank,") id_region=",id_region,
c     &        "origine(id_region)=",origines(id_region)
         call type_subRegion(dim,
     &     regions(L,1,B_),
     &     regions(L,1,id_region),
     &     MPI_COMPLEX16,types(id_region))
         if(types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_TYPE_COMMIT(types(id_region),iErr)
         endif
      enddo
c      print *,STEP_Rank,") types=", types
      SELECT CASE(algorithm)
      case (BLOCKING1)
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even_Merge_C16'
            call AlltoAll_even_Merge_C16(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd_Merge_C16'
            call AlltoAll_odd_Merge_C16(STEP_Size,types,origines
     >,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      case DEFAULT
         if(mod(STEP_SIZE,2).EQ.0)then
c      print *,STEP_Rank,') AlltoAll_even2_Merge_C16'
            call AlltoAll_even2_Merge_C16(STEP_Size,types,origin
     >es,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         else
c      print *,STEP_Rank,') AlltoAll_odd2_Merge_C16'
            call AlltoAll_odd2_Merge_C16(STEP_Size,types,origine
     >s,
     &           STEP_SizeRegion(dim,regions(L,1,B_)),array,tag
     &           ,dim,nb_regions,regions,initial,buffer)
         endif
      end select
      ! liberation de la memoire pour les type MPI
      do id_region=1,nb_regions
         if (types(id_region).NE.MPI_DATATYPE_NULL) then
            call MPI_Type_free(types(id_region),iErr);
         endif
      enddo
      end
      subroutine AlltoAll_even_Merge_C16(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'mpif.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*16 initial(1:size),buffer(1:size)
      integer STEP_Rank,id_data
      integer data_send,data_recv,incr
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      CALL STEP_Get_rank(STEP_Rank)
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      if (comm_node(0).GT.STEP_Rank) then ! cas STEP_Rank pair
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         data_recv=STEP_Rank
      else
         if(types(comm_node(0)).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(comm_node(0))),1,
     &           types(comm_node(0)),comm_node(0),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,comm_node(0)+1,
     &           size,initial,buffer,array)
         endif
         if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(STEP_Rank)),1,
     &           types(STEP_Rank),comm_node(0),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         data_recv=STEP_Rank-1
      endif
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         if (comm_node(m).GT.STEP_Rank) then ! cas STEP_Rank pair
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         endif
         if(types(data_recv).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &     nb_regions,regions,data_recv+1,
     &     size,initial,buffer,array)
         endif
         if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &     nb_regions,regions,data_recv+2,
     &     size,initial,buffer,array)
         endif
         else
            if(types(data_recv).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv)),1,
     &              types(data_recv),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C16(dim,
     &              nb_regions,regions,data_recv+1,
     &              size,initial,buffer,array)
            endif
            if(types(data_recv+1).NE.MPI_DATATYPE_NULL) then
               call MPI_RECV(buffer(origines(data_recv+1)),1,
     &              types(data_recv+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C16(dim,
     &              nb_regions,regions,data_recv+2,
     &              size,initial,buffer,array)
            endif
            if(types(data_send).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send)),1,
     &              types(data_send),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
            if(types(data_send+1).NE.MPI_DATATYPE_NULL) then
               call MPI_SEND(array(origines(data_send+1)),1,
     &              types(data_send+1),comm_node(m),tag,
     &              MPI_COMM_WORLD,iErr)
            endif
         endif
      enddo
      end
      subroutine AlltoAll_even2_Merge_C16(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:STEP_MAX_NBNODE-1)
      integer origines(0:STEP_MAX_NBNODE-1),size
      complex*16 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*16 initial(1:size),buffer(1:size)
      integer id_data
      integer data_send,data_recv,incr
      integer comm
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      integer step,m
      integer*4 comm_node(0:1)
      logical rank_even,step_even
      step=0
      if (mod(STEP_Rank,2).EQ.0) then !calcule du noeud avec lequel on communique a l'etape step
         comm_node(0)=STEP_Rank+1
         comm_node(1)=STEP_Rank+nb_node-1
         rank_even=.TRUE.
         data_recv=STEP_Rank
      else
         comm_node(0)=STEP_Rank+nb_node-1
         comm_node(1)=STEP_Rank+1
         rank_even=.FALSE.
         data_recv=STEP_Rank-1
      endif
      comm_node(0)=mod(comm_node(0),nb_node) ! cas d'une etape pair
      comm_node(1)=mod(comm_node(1),nb_node) ! cas d'une etape impair
      comm=0
      if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
         comm=comm+2
      endif
      if (types(comm_node(0)).NE.MPI_DATATYPE_NULL)then
         comm=comm+1
      endif
      select case(comm)
      case(3)
         call MPI_SENDRECV(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_C16(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      case(2)
         call MPI_SEND(array(origines(STEP_Rank)),1,
     &        types(STEP_Rank),comm_node(0),tag,
     &        MPI_COMM_WORLD,iErr)
      case(1)
         call MPI_RECV(buffer(origines(comm_node(0))),1,
     &        types(comm_node(0)),comm_node(0),tag,
     &        MPI_COMM_WORLD,STATUS,iErr)
         call MergeRegion_C16(dim,
     &        nb_regions,regions,comm_node(0)+1,
     &        size,initial,buffer,array)
      end select
      m=0
      step_even=.TRUE.
      do step=1,nb_node/2-1
         m=1-m
         step_even=.NOT. step_even
         data_send=data_recv
         if(rank_even.EQV.step_even)then
            data_recv=mod(STEP_Rank+step,nb_node)
         else
            data_recv=mod(STEP_Rank-step-1+nb_node,nb_node)
         endif
         comm=0
         if (types(data_send).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send)),1,
     &           types(data_send),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv)),1,
     &           types(data_recv),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,data_recv+1,
     &           size,initial,buffer,array)
         end select
         comm=0
         if (types(data_send+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+2
         endif
         if (types(data_recv+1).NE.MPI_DATATYPE_NULL)then
            comm=comm+1
         endif
         select case(comm)
         case(3)
            call MPI_SENDRECV(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         case(2)
            call MPI_SEND(array(origines(data_send+1)),1,
     &           types(data_send+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,iErr)
         case(1)
            call MPI_RECV(buffer(origines(data_recv+1)),1,
     &           types(data_recv+1),comm_node(m),tag,
     &           MPI_COMM_WORLD,STATUS,iErr)
            call MergeRegion_C16(dim,
     &           nb_regions,regions,data_recv+2,
     &           size,initial,buffer,array)
         end select
      enddo
      end
      subroutine AlltoAll_odd_Merge_C16(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*16 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            if(node.LT.STEP_Rank) then
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_C16(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
            else
               if(types(node).NE.MPI_DATATYPE_NULL) then
                  call MPI_RECV(buffer(origines(node)),1,
     &                 types(node),node,tag,
     &                 MPI_COMM_WORLD,STATUS,iErr)
                  call MergeRegion_C16(dim,
     &                 nb_regions,regions,node+1,
     &                 size,initial,buffer,array)
               endif
               if(types(STEP_Rank).NE.MPI_DATATYPE_NULL) then
                  call MPI_SEND(array(origines(STEP_Rank)),1,
     &                 types(STEP_Rank),node,tag,
     &                 MPI_COMM_WORLD,iErr)
               endif
            endif
         endif
      enddo
      end
      subroutine AlltoAll_odd2_Merge_C16(nb_node,types,origines,
     &     size,array,tag ,dim,nb_regions,regions,initial,buffer)
      implicit none
      include 'STEP.h'
      integer nb_node
      integer*4 types(0:nb_node-1)
      integer origines(0:nb_node-1),size
      complex*16 array(size)
      integer*4 tag
      integer dim
      integer L,U,B_
      parameter (L=1,U=2,B_=0)
      integer nb_regions        !nombre d'élément du tableau région +1
      integer regions(L:U,1:dim,B_:nb_regions) !description de l'espace d'indice des différentes régions
                                !regions(:,:,B_) decrit l'espace d'indice du tableau array non linéarisé
      complex*16 initial(1:size),buffer(1:size)
      integer comm_node(0:STEP_MAX_NBNODE-1),alias,step,comm
      integer*4 node
      integer*4 STATUS(MPI_STATUS_SIZE),iErr
      comm_node(0)=MPI_PROC_NULL
      do node=1,nb_node-1
         comm_node(node)=2*nb_node-node
      enddo
      do step=0,nb_node-1
         alias=mod(STEP_Rank+step,nb_node)
         if (alias.NE.0)then
            node=mod(comm_node(alias)-step,nb_node)
            comm=0
            if (types(STEP_Rank).NE.MPI_DATATYPE_NULL)then
               comm=comm+2
            endif
            if (types(node).NE.MPI_DATATYPE_NULL)then
               comm=comm+1
            endif
            select case(comm)
            case(3)
               call MPI_SENDRECV(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C16(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            case(2)
               call MPI_SEND(
     &              array(origines(STEP_Rank)),1,
     &              types(STEP_Rank),node,tag,
     &              MPI_COMM_WORLD,iErr)
            case(1)
               call MPI_RECV(
     &              buffer(origines(node)),1,
     &              types(node),node,tag,
     &              MPI_COMM_WORLD,STATUS,iErr)
               call MergeRegion_C16(dim,
     &              nb_regions,regions,node+1,
     &              size,initial,buffer,array)
            end select
         endif
      enddo
      end

###################### master/src/STEP.h ######################
* Copyright 2007, 2008 Alain Muller, Frederique Silber-Chaussumier
*
*This file is part of STEP.
*
*The program is distributed under the terms of the GNU General Public
*License.



      include 'mpif.h'

      integer STEP_MAX_NBNODE
      parameter (STEP_MAX_NBNODE = 16)

      integer MAX_NB_LOOPSLICES
      parameter (MAX_NB_LOOPSLICES = STEP_MAX_NBNODE)

      integer STEP_MAX_DIM
      parameter (STEP_MAX_DIM = 10)

      integer STEP_MAX_NBREQ
      parameter (STEP_MAX_NBREQ = 2*STEP_MAX_NBNODE*10)


      integer*4 STEP_size
      integer*4 STEP_rank
      COMMON /STEP/ STEP_size,STEP_rank

      integer STEP_SizeRegion

      integer*4 STEP_Status(1:MPI_STATUS_SIZE)
      integer*4 STEP_NbRequest

      integer IDX_SLICE_LOW,IDX_SLICE_UP,STEP_IDX
      parameter (IDX_SLICE_LOW = 1,IDX_SLICE_UP=2)

      integer STEP_NONBLOCKING,STEP_BLOCKING1,STEP_BLOCKING2
      parameter (STEP_NONBLOCKING=0,STEP_BLOCKING1=1,STEP_BLOCKING2=2)

      integer STEP_SUM,STEP_PROD
      integer STEP_AND,STEP_OR
      integer STEP_MIN,STEP_MAX
      integer STEP_IAND,STEP_IOR,STEP_IEOR
      parameter (STEP_SUM=MPI_SUM,STEP_PROD=MPI_PROD)
      parameter (STEP_AND=MPI_LAND,STEP_OR=MPI_LOR)
      parameter (STEP_MIN=MPI_MIN,STEP_MAX=MPI_MAX)
      parameter (STEP_IAND=MPI_BAND,STEP_IOR=MPI_BOR,STEP_IEOR=MPI_BXOR)
###################### master/src/master.f ######################
!!
!! file for master.f
!!
!
!
! Example of a master directive
!
! 2008
! Creation: A. Muller, 2008
!
      PROGRAM MASTER
      implicit none
      INTEGER N
      PARAMETER (N=10)
      INTEGER I,J,A(N)
      CALL STEP_INIT

      I = -3                                                            0015
C     !$omp parallel
      CALL MASTER_PAR1(J, N, A, I)
C     !$omp end parallel
      CALL STEP_FINALIZE

      END
